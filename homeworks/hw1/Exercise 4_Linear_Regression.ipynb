{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Python: Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with one variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part of the exercise, we're tasked with implementing linear regression with one variable to predict profits for a food truck. Suppose you are the CEO of a restaurant franchise and are considering different cities for opening a new outlet. The chain already has trucks in various cities and you have data for profits and populations from the cities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing some libraries and examining the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data from the CSV file using Panda library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Population</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.1101</td>\n",
       "      <td>17.5920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.5277</td>\n",
       "      <td>9.1302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.5186</td>\n",
       "      <td>13.6620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0032</td>\n",
       "      <td>11.8540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.8598</td>\n",
       "      <td>6.8233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Population   Profit\n",
       "0      6.1101  17.5920\n",
       "1      5.5277   9.1302\n",
       "2      8.5186  13.6620\n",
       "3      7.0032  11.8540\n",
       "4      5.8598   6.8233"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.getcwd() + '\\\\data\\\\ex1data1.txt'\n",
    "data = pd.read_csv(path, header=None, names=['Population', 'Profit'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Population</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>97.000000</td>\n",
       "      <td>97.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.159800</td>\n",
       "      <td>5.839135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.869884</td>\n",
       "      <td>5.510262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.026900</td>\n",
       "      <td>-2.680700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.707700</td>\n",
       "      <td>1.986900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.589400</td>\n",
       "      <td>4.562300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.578100</td>\n",
       "      <td>7.046700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>22.203000</td>\n",
       "      <td>24.147000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Population     Profit\n",
       "count   97.000000  97.000000\n",
       "mean     8.159800   5.839135\n",
       "std      3.869884   5.510262\n",
       "min      5.026900  -2.680700\n",
       "25%      5.707700   1.986900\n",
       "50%      6.589400   4.562300\n",
       "75%      8.578100   7.046700\n",
       "max     22.203000  24.147000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot it to get a better idea of what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Population', ylabel='Profit'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.plot(kind='scatter', x='Population', y='Profit', figsize=(12,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement linear regression using gradient descent to minimize the cost function.  The equations implemented in the following code samples are detailed in \"ex1.pdf\" in the \"exercises\" folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll create a function to compute the cost of a given solution (characterized by the parameters theta). The cost function is the Mean Sqaured error in matrix form: \n",
    "\n",
    "$$ MSE(\\theta) = \\frac{1}{N}\\sum_n^N [ y_n-x_n^T*\\theta]^2 $$\n",
    "\n",
    "where $\\theta$ and $x_n$ are vectors\n",
    "\n",
    "__Hint__: Use the matrix form of the cost function and make use of numpy operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost(x, y, theta):\n",
    "    # Compute the cost function\n",
    "    N = x.shape[0]\n",
    "    return 1/N*sum((y - x@theta)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a column of ones to the training set so we can use a vectorized solution to computing the cost and gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ones</th>\n",
       "      <th>Population</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6.1101</td>\n",
       "      <td>17.5920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5.5277</td>\n",
       "      <td>9.1302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>8.5186</td>\n",
       "      <td>13.6620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>7.0032</td>\n",
       "      <td>11.8540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5.8598</td>\n",
       "      <td>6.8233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ones  Population   Profit\n",
       "0     1      6.1101  17.5920\n",
       "1     1      5.5277   9.1302\n",
       "2     1      8.5186  13.6620\n",
       "3     1      7.0032  11.8540\n",
       "4     1      5.8598   6.8233"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.insert(0, 'Ones', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do some variable initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set X (training data) and y (target variable)\n",
    "cols = data.shape[1]\n",
    "X = data.iloc[:,0:cols-1]\n",
    "y = data.iloc[:,cols-1:cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look to make sure X (training set) and y (target variable) look correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ones</th>\n",
       "      <th>Population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6.1101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5.5277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>8.5186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>7.0032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5.8598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ones  Population\n",
       "0     1      6.1101\n",
       "1     1      5.5277\n",
       "2     1      8.5186\n",
       "3     1      7.0032\n",
       "4     1      5.8598"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.5920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.1302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.6620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.8540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.8233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Profit\n",
       "0  17.5920\n",
       "1   9.1302\n",
       "2  13.6620\n",
       "3  11.8540\n",
       "4   6.8233"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert X and Y to numpy array for better manipulation. Initiliaze Theta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(X.values)\n",
    "y = np.array(y.values).flatten()\n",
    "theta = np.array([0,0])\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the shape of our matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((97, 2), (2,), (97,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, theta.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the cost for our initial solution (0 values for theta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.14546775491131"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeCost(X, y, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good.  Now we need to define a function to perform gradient descent on the parameters theta using the update rules. Write first a function that computes the gradient of a matrix and then use it in the gradientDescent function.\n",
    "\n",
    "The gradient descent formula is:\n",
    "\n",
    "$$\\theta^{t+1} = \\theta^{t} - \\alpha*\\nabla MSE(\\theta^{t})$$\n",
    "\n",
    "where $\\nabla MSE(\\theta^{t})$ is the gradient of the cost function at $\\theta^{t}$\n",
    "\n",
    "__Hint__: Use the matrix form of the gradient and make use of numpy operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, X, w): # X au lieu de tx jsp ce que leur tx représente, on va dire que w c theta\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute gradient and loss\n",
    "    N = y.shape[0]\n",
    "    grad = -2/N * np.array([sum((y - X@w)*X[:,0]), sum((y - X@w)*X[:,1])])\n",
    "    # grad = 2/N * X.T@(X@w - y) -> better chatGPT version\n",
    "    loss = computeCost(X,y,w) # loss = cost\n",
    "    # ***************************************************\n",
    "    return grad, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X, y, theta, alpha,max_iters):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [theta]\n",
    "    cost = np.zeros(max_iters)\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        grad, loss = compute_gradient(y,X,theta)\n",
    "        # ***************************************************\n",
    "        \n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        theta = theta - alpha * grad # element-wise mutliplication here\n",
    "        # ***************************************************\n",
    "        # store w and loss\n",
    "        ws.append(theta)\n",
    "        cost[n_iter] = loss\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=theta[0], w1=theta[1]))\n",
    "\n",
    "    return theta, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize some additional variables - the learning rate alpha, and the number of iterations to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "iters = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the gradient descent algorithm to fit our parameters theta to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/999): loss=64.14546775491131, w0=0.1167827010309278, w1=1.3065769949111345\n",
      "Gradient Descent(1/999): loss=33.53928474333497, w0=0.018001608779719583, w1=0.4668851404174774\n",
      "Gradient Descent(2/999): loss=20.81515950353795, w0=0.058230490259482376, w1=1.0103985192819032\n",
      "Gradient Descent(3/999): loss=15.51836696060469, w0=0.008955584732491098, w1=0.6624640652822504\n",
      "Gradient Descent(4/999): loss=13.3065750692935, w0=0.017447688470966963, w1=0.8890358067492414\n",
      "Gradient Descent(5/999): loss=12.376211646075983, w0=-0.011205651785773731, w1=0.7453450091050499\n",
      "Gradient Descent(6/999): loss=11.978169038728405, w0=-0.01583616182503814, w1=0.840270271487082\n",
      "Gradient Descent(7/999): loss=11.801307545851571, w0=-0.03586548478321538, w1=0.7814054019591586\n",
      "Gradient Descent(8/999): loss=11.716395840326456, w0=-0.04588771003475009, w1=0.8216458872863021\n",
      "Gradient Descent(9/999): loss=11.669757570402808, w0=-0.0622765770247026, w1=0.7980072763515389\n",
      "Gradient Descent(10/999): loss=11.639097868446486, w0=-0.07447993992474645, w1=0.8155287856125966\n",
      "Gradient Descent(11/999): loss=11.61515605455456, w0=-0.089298675792157, w1=0.8065154361246225\n",
      "Gradient Descent(12/999): loss=11.594085154771328, w0=-0.10235009435917991, w1=0.8145948887823523\n",
      "Gradient Descent(13/999): loss=11.574286762109516, w0=-0.11645901891079326, w1=0.8116502947008781\n",
      "Gradient Descent(14/999): loss=11.555096407741408, w0=-0.12980521899565406, w1=0.8158022520353791\n",
      "Gradient Descent(15/999): loss=11.536237534751155, w0=-0.14356207790797887, w1=0.815372541889782\n",
      "Gradient Descent(16/999): loss=11.517594708120662, w0=-0.15697367266513632, w1=0.8178874980936855\n",
      "Gradient Descent(17/999): loss=11.499119418305689, w0=-0.17052746631980287, w1=0.8184966258951272\n",
      "Gradient Descent(18/999): loss=11.48079095706981, w0=-0.18390959132206014, w1=0.8203259705114381\n",
      "Gradient Descent(19/999): loss=11.462600167377458, w0=-0.19732261554827574, w1=0.821360907108657\n",
      "Gradient Descent(20/999): loss=11.444542693942983, w0=-0.21063627680288677, w1=0.8228998372158081\n",
      "Gradient Descent(21/999): loss=11.426616179842407, w0=-0.2239348120701722, w1=0.824106004016073\n",
      "Gradient Descent(22/999): loss=11.408819102107845, w0=-0.237164218229248, w1=0.8255187039129651\n",
      "Gradient Descent(23/999): loss=11.39115028806755, w0=-0.25035958323751545, w1=0.8267904031447671\n",
      "Gradient Descent(24/999): loss=11.373608714431436, w0=-0.2634985771734507, w1=0.8281451065334088\n",
      "Gradient Descent(25/999): loss=11.3561934238139, w0=-0.27659587340488007, w1=0.8294384772326225\n",
      "Gradient Descent(26/999): loss=11.33890349003421, w0=-0.2896422966363097, w1=0.8307635647986902\n",
      "Gradient Descent(27/999): loss=11.321738003676494, w0=-0.3026440403935427, w1=0.8320604294011863\n",
      "Gradient Descent(28/999): loss=11.304696066064787, w0=-0.3155973923913, w1=0.8333677287388969\n",
      "Gradient Descent(29/999): loss=11.28777678673345, w0=-0.3285050233718192, w1=0.8346605768984673\n",
      "Gradient Descent(30/999): loss=11.270979282349435, w0=-0.34136548938097727, w1=0.8359550400503488\n",
      "Gradient Descent(31/999): loss=11.254302676237955, w0=-0.3541799972784866, w1=0.8372407917250404\n",
      "Gradient Descent(32/999): loss=11.237746098158809, w0=-0.36694804454834873, w1=0.8385245147143746\n",
      "Gradient Descent(33/999): loss=11.22130868418708, w0=-0.379670229329781, w1=0.8398019296894986\n",
      "Gradient Descent(34/999): loss=11.204989576637418, w0=-0.39234643942986497, w1=0.8410758220203124\n",
      "Gradient Descent(35/999): loss=11.188787924006496, w0=-0.40497701946076675, w1=0.8423444239654391\n",
      "Gradient Descent(36/999): loss=11.172702880923316, w0=-0.4175620186540874, w1=0.8436089020451878\n",
      "Gradient Descent(37/999): loss=11.156733608102838, w0=-0.43010167562824425, w1=0.8448685316272173\n",
      "Gradient Descent(38/999): loss=11.140879272301376, w0=-0.4425961059721869, w1=0.8461238067793512\n",
      "Gradient Descent(39/999): loss=11.12513904627266, w0=-0.4550455035929783, w1=0.8473744360516899\n",
      "Gradient Descent(40/999): loss=11.10951210872455, w0=-0.46745001095608246, w1=0.8486206341426099\n",
      "Gradient Descent(41/999): loss=11.09399764427609, w0=-0.47980980271557033, w1=0.8498622894543949\n",
      "Gradient Descent(42/999): loss=11.07859484341496, w0=-0.4921250318201305, w1=0.8510995005820562\n",
      "Gradient Descent(43/999): loss=11.063302902455256, w0=-0.5043958642497893, w1=0.8523322305607662\n",
      "Gradient Descent(44/999): loss=11.048121023495462, w0=-0.5166224566324605, w1=0.8535605296946277\n",
      "Gradient Descent(45/999): loss=11.033048414376907, w0=-0.528804970672928, w1=0.8547843919497466\n",
      "Gradient Descent(46/999): loss=11.018084288642374, w0=-0.5409435638571725, w1=0.8560038475054581\n",
      "Gradient Descent(47/999): loss=11.003227865495083, w0=-0.5530383954466019, w1=0.8572189031078419\n",
      "Gradient Descent(48/999): loss=10.988478369757953, w0=-0.5650896226183294, w1=0.8584295805106528\n",
      "Gradient Descent(49/999): loss=10.973835031833172, w0=-0.5770974029560515, w1=0.8596358917014932\n",
      "Gradient Descent(50/999): loss=10.959297087661968, w0=-0.5890618928481195, w1=0.8608378548682327\n",
      "Gradient Descent(51/999): loss=10.94486377868485, w0=-0.6009832485233054, w1=0.8620354841094084\n",
      "Gradient Descent(52/999): loss=10.930534351801937, w0=-0.6128616253866305, w1=0.8632287960659379\n",
      "Gradient Descent(53/999): loss=10.91630805933373, w0=-0.6246971784507468, w1=0.8644178056474903\n",
      "Gradient Descent(54/999): loss=10.902184158982017, w0=-0.6364900620612518, w1=0.8656025287870629\n",
      "Gradient Descent(55/999): loss=10.888161913791253, w0=-0.6482404300770325, w1=0.8667829806660605\n",
      "Gradient Descent(56/999): loss=10.874240592110008, w0=-0.6599484357573424, w1=0.8679591768585192\n",
      "Gradient Descent(57/999): loss=10.860419467552845, w0=-0.6716142318378706, w1=0.869131132594013\n",
      "Gradient Descent(58/999): loss=10.846697818962397, w0=-0.6832379704849979, w1=0.8702988632330222\n",
      "Gradient Descent(59/999): loss=10.833074930371726, w0=-0.6948198033285464, w1=0.8714623839608985\n",
      "Gradient Descent(60/999): loss=10.819550090967038, w0=-0.7063598814439305, w1=0.872621709985424\n",
      "Gradient Descent(61/999): loss=10.806122595050484, w0=-0.7178583553669052, w1=0.8737768564098157\n",
      "Gradient Descent(62/999): loss=10.792791742003423, w0=-0.7293153750872956, w1=0.8749278383148953\n",
      "Gradient Descent(63/999): loss=10.779556836249832, w0=-0.7407310900562595, w1=0.8760746707064573\n",
      "Gradient Descent(64/999): loss=10.766417187220014, w0=-0.7521056491848175, w1=0.8772173685495103\n",
      "Gradient Descent(65/999): loss=10.75337210931457, w0=-0.7634392008479992, w1=0.878355946746532\n",
      "Gradient Descent(66/999): loss=10.740420921868619, w0=-0.7747318928853584, w1=0.8794904201518022\n",
      "Gradient Descent(67/999): loss=10.727562949116308, w0=-0.7859838726038169, w1=0.8806208035624858\n",
      "Gradient Descent(68/999): loss=10.71479752015551, w0=-0.7971952867789961, w1=0.881747111724697\n",
      "Gradient Descent(69/999): loss=10.702123968912844, w0=-0.808366281657512, w1=0.8828693593299076\n",
      "Gradient Descent(70/999): loss=10.689541634108918, w0=-0.8194970029586376, w1=0.8839875610175774\n",
      "Gradient Descent(71/999): loss=10.677049859223834, w0=-0.8305875958763616, w1=0.8851017313737736\n",
      "Gradient Descent(72/999): loss=10.664647992462909, w0=-0.8416382050811808, w1=0.8862118849323739\n",
      "Gradient Descent(73/999): loss=10.652335386722672, w0=-0.852648974722053, w1=0.8873180361746037\n",
      "Gradient Descent(74/999): loss=10.640111399557114, w0=-0.8636200484282348, w1=0.8884201995296449\n",
      "Gradient Descent(75/999): loss=10.62797539314416, w0=-0.8745515693111822, w1=0.8895183893745531\n",
      "Gradient Descent(76/999): loss=10.615926734252382, w0=-0.8854436799664003, w1=0.8906126200346214\n",
      "Gradient Descent(77/999): loss=10.603964794207961, w0=-0.8962965224753145, w1=0.8917029057834516\n",
      "Gradient Descent(78/999): loss=10.592088948861905, w0=-0.9071102384071166, w1=0.892789260843217\n",
      "Gradient Descent(79/999): loss=10.580298578557429, w0=-0.9178849688206161, w1=0.8938716993847967\n",
      "Gradient Descent(80/999): loss=10.568593068097691, w0=-0.9286208542660772, w1=0.8949502355279949\n",
      "Gradient Descent(81/999): loss=10.556971806713634, w0=-0.9393180347870544, w1=0.8960248833417019\n",
      "Gradient Descent(82/999): loss=10.545434188032143, w0=-0.9499766499222179, w1=0.8970956568440931\n",
      "Gradient Descent(83/999): loss=10.533979610044433, w0=-0.9605968387071743, w1=0.8981625700028018\n",
      "Gradient Descent(84/999): loss=10.522607475074569, w0=-0.9711787396762802, w1=0.8992256367351078\n",
      "Gradient Descent(85/999): loss=10.51131718974835, w0=-0.9817224908644494, w1=0.9002848709081144\n",
      "Gradient Descent(86/999): loss=10.500108164962299, w0=-0.9922282298089532, w1=0.9013402863389335\n",
      "Gradient Descent(87/999): loss=10.488979815852954, w0=-1.0026960935512148, w1=0.9023918967948614\n",
      "Gradient Descent(88/999): loss=10.477931561766338, w0=-1.013126218638597, w1=0.9034397159935627\n",
      "Gradient Descent(89/999): loss=10.466962826227663, w0=-1.0235187411261826, w1=0.9044837576032457\n",
      "Gradient Descent(90/999): loss=10.456073036911263, w0=-1.0338737965785503, w1=0.9055240352428428\n",
      "Gradient Descent(91/999): loss=10.445261625610708, w0=-1.0441915200715424, w1=0.9065605624821879\n",
      "Gradient Descent(92/999): loss=10.434528028209177, w0=-1.054472046194027, w1=0.9075933528421922\n",
      "Gradient Descent(93/999): loss=10.423871684649999, w0=-1.064715509049653, w1=0.9086224197950221\n",
      "Gradient Descent(94/999): loss=10.413292038907475, w0=-1.0749220422586006, w1=0.909647776764274\n",
      "Gradient Descent(95/999): loss=10.40278853895782, w0=-1.085091778959323, w1=0.9106694371251496\n",
      "Gradient Descent(96/999): loss=10.392360636750375, w0=-1.0952248518102847, w1=0.9116874142046306\n",
      "Gradient Descent(97/999): loss=10.382007788179024, w0=-1.10532139299169, w1=0.9127017212816524\n",
      "Gradient Descent(98/999): loss=10.371729453053765, w0=-1.115381534207209, w1=0.913712371587277\n",
      "Gradient Descent(99/999): loss=10.361525095072604, w0=-1.1254054066856944, w1=0.914719378304866\n",
      "Gradient Descent(100/999): loss=10.351394181793477, w0=-1.1353931411828937, w1=0.915722754570253\n",
      "Gradient Descent(101/999): loss=10.341336184606524, w0=-1.145344867983155, w1=0.9167225134719134\n",
      "Gradient Descent(102/999): loss=10.331350578706479, w0=-1.1552607169011264, w1=0.917718668051137\n",
      "Gradient Descent(103/999): loss=10.321436843065328, w0=-1.1651408172834494, w1=0.9187112313021967\n",
      "Gradient Descent(104/999): loss=10.311594460405091, w0=-1.1749852980104458, w1=0.9197002161725192\n",
      "Gradient Descent(105/999): loss=10.301822917170814, w0=-1.1847942874977995, w1=0.9206856355628522\n",
      "Gradient Descent(106/999): loss=10.292121703503833, w0=-1.1945679136982308, w1=0.921667502327435\n",
      "Gradient Descent(107/999): loss=10.28249031321509, w0=-1.2043063041031665, w1=0.9226458292741638\n",
      "Gradient Descent(108/999): loss=10.272928243758805, w0=-1.2140095857444018, w1=0.9236206291647606\n",
      "Gradient Descent(109/999): loss=10.263434996206199, w0=-1.2236778851957582, w1=0.9245919147149386\n",
      "Gradient Descent(110/999): loss=10.25401007521946, w0=-1.2333113285747344, w1=0.9255596985945685\n",
      "Gradient Descent(111/999): loss=10.244652989025946, w0=-1.242910041544151, w1=0.9265239934278436\n",
      "Gradient Descent(112/999): loss=10.235363249392456, w0=-1.2524741493137905, w1=0.9274848117934449\n",
      "Gradient Descent(113/999): loss=10.226140371599822, w0=-1.26200377664203, w1=0.9284421662247044\n",
      "Gradient Descent(114/999): loss=10.216983874417563, w0=-1.2714990478374684, w1=0.9293960692097694\n",
      "Gradient Descent(115/999): loss=10.207893280078835, w0=-1.2809600867605486, w1=0.9303465331917646\n",
      "Gradient Descent(116/999): loss=10.198868114255417, w0=-1.290387016825173, w1=0.9312935705689555\n",
      "Gradient Descent(117/999): loss=10.189907906033055, w0=-1.299779961000313, w1=0.9322371936949089\n",
      "Gradient Descent(118/999): loss=10.181012187886813, w0=-1.3091390418116131, w1=0.9331774148786547\n",
      "Gradient Descent(119/999): loss=10.17218049565674, w0=-1.31846438134299, w1=0.9341142463848467\n",
      "Gradient Descent(120/999): loss=10.1634123685236, w0=-1.3277561012382237, w1=0.9350477004339222\n",
      "Gradient Descent(121/999): loss=10.154707348984878, w0=-1.3370143227025457, w1=0.9359777892022608\n",
      "Gradient Descent(122/999): loss=10.146064982830849, w0=-1.346239166504219, w1=0.9369045248223454\n",
      "Gradient Descent(123/999): loss=10.137484819120935, w0=-1.3554307529761143, w1=0.9378279193829172\n",
      "Gradient Descent(124/999): loss=10.128966410160134, w0=-1.3645892020172787, w1=0.9387479849291366\n",
      "Gradient Descent(125/999): loss=10.120509311475708, w0=-1.3737146330945007, w1=0.939664733462738\n",
      "Gradient Descent(126/999): loss=10.112113081793916, w0=-1.3828071652438678, w1=0.9405781769421877\n",
      "Gradient Descent(127/999): loss=10.10377728301707, w0=-1.39186691707232, w1=0.9414883272828394\n",
      "Gradient Descent(128/999): loss=10.095501480200632, w0=-1.400894006759196, w1=0.9423951963570893\n",
      "Gradient Descent(129/999): loss=10.08728524153051, w0=-1.4098885520577757, w1=0.9432987959945319\n",
      "Gradient Descent(130/999): loss=10.079128138300563, w0=-1.418850670296816, w1=0.9441991379821133\n",
      "Gradient Descent(131/999): loss=10.071029744890225, w0=-1.4277804783820809, w1=0.9450962340642857\n",
      "Gradient Descent(132/999): loss=10.062989638742247, w0=-1.4366780927978666, w1=0.9459900959431599\n",
      "Gradient Descent(133/999): loss=10.055007400340722, w0=-1.4455436296085213, w1=0.9468807352786586\n",
      "Gradient Descent(134/999): loss=10.047082613189152, w0=-1.454377204459959, w1=0.9477681636886686\n",
      "Gradient Descent(135/999): loss=10.03921486378871, w0=-1.463178932581168, w1=0.9486523927491913\n",
      "Gradient Descent(136/999): loss=10.031403741616698, w0=-1.4719489287857137, w1=0.9495334339944949\n",
      "Gradient Descent(137/999): loss=10.023648839105105, w0=-1.4806873074732372, w1=0.9504112989172643\n",
      "Gradient Descent(138/999): loss=10.015949751619337, w0=-1.4893941826309465, w1=0.9512859989687503\n",
      "Gradient Descent(139/999): loss=10.008306077437108, w0=-1.498069667835104, w1=0.9521575455589213\n",
      "Gradient Descent(140/999): loss=10.000717417727511, w0=-1.5067138762525079, w1=0.9530259500566092\n",
      "Gradient Descent(141/999): loss=9.993183376530135, w0=-1.5153269206419682, w1=0.9538912237896597\n",
      "Gradient Descent(142/999): loss=9.985703560734482, w0=-1.5239089133557784, w1=0.954753378045079\n",
      "Gradient Descent(143/999): loss=9.978277580059409, w0=-1.5324599663411798, w1=0.9556124240691823\n",
      "Gradient Descent(144/999): loss=9.970905047032783, w0=-1.5409801911418226, w1=0.9564683730677386\n",
      "Gradient Descent(145/999): loss=9.963585576971248, w0=-1.549469698899221, w1=0.9573212362061191\n",
      "Gradient Descent(146/999): loss=9.95631878796017, w0=-1.5579286003542026, w1=0.9581710246094405\n",
      "Gradient Descent(147/999): loss=9.949104300833712, w0=-1.566357005848353, w1=0.9590177493627127\n",
      "Gradient Descent(148/999): loss=9.941941739155018, w0=-1.5747550253254552, w1=0.9598614215109802\n",
      "Gradient Descent(149/999): loss=9.934830729196587, w0=-1.583122768332924, w1=0.9607020520594703\n",
      "Gradient Descent(150/999): loss=9.927770899920832, w0=-1.591460344023235, w1=0.9615396519737319\n",
      "Gradient Descent(151/999): loss=9.920761882960592, w0=-1.5997678611553476, w1=0.9623742321797832\n",
      "Gradient Descent(152/999): loss=9.913803312600054, w0=-1.6080454280961247, w1=0.9632058035642502\n",
      "Gradient Descent(153/999): loss=9.906894825755563, w0=-1.6162931528217457, w1=0.964034376974512\n",
      "Gradient Descent(154/999): loss=9.900036061956722, w0=-1.6245111429191155, w1=0.9648599632188396\n",
      "Gradient Descent(155/999): loss=9.893226663327603, w0=-1.632699505587267, w1=0.9656825730665393\n",
      "Gradient Descent(156/999): loss=9.886466274568011, w0=-1.6408583476387608, w1=0.9665022172480903\n",
      "Gradient Descent(157/999): loss=9.87975454293501, w0=-1.6489877755010771, w1=0.9673189064552886\n",
      "Gradient Descent(158/999): loss=9.873091118224474, w0=-1.657087895218005, w1=0.968132651341382\n",
      "Gradient Descent(159/999): loss=9.866475652752866, w0=-1.6651588124510253, w1=0.9689434625212134\n",
      "Gradient Descent(160/999): loss=9.859907801339016, w0=-1.673200632480689, w1=0.9697513505713551\n",
      "Gradient Descent(161/999): loss=9.853387221286182, w0=-1.6812134602079902, w1=0.9705563260302507\n",
      "Gradient Descent(162/999): loss=9.846913572364144, w0=-1.6891974001557353, w1=0.9713583993983489\n",
      "Gradient Descent(163/999): loss=9.840486516791438, w0=-1.6971525564699057, w1=0.9721575811382439\n",
      "Gradient Descent(164/999): loss=9.834105719217755, w0=-1.7050790329210166, w1=0.9729538816748093\n",
      "Gradient Descent(165/999): loss=9.827770846706416, w0=-1.7129769329054707, w1=0.9737473113953352\n",
      "Gradient Descent(166/999): loss=9.821481568717036, w0=-1.7208463594469066, w1=0.9745378806496638\n",
      "Gradient Descent(167/999): loss=9.815237557088238, w0=-1.728687415197543, w1=0.9753255997503236\n",
      "Gradient Descent(168/999): loss=9.809038486020565, w0=-1.7365002024395182, w1=0.9761104789726653\n",
      "Gradient Descent(169/999): loss=9.802884032059445, w0=-1.7442848230862231, w1=0.9768925285549936\n",
      "Gradient Descent(170/999): loss=9.796773874078372, w0=-1.7520413786836315, w1=0.9776717586987043\n",
      "Gradient Descent(171/999): loss=9.790707693262053, w0=-1.7597699704116248, w1=0.9784481795684127\n",
      "Gradient Descent(172/999): loss=9.784685173089885, w0=-1.767470699085311, w1=0.9792218012920907\n",
      "Gradient Descent(173/999): loss=9.778705999319353, w0=-1.775143665156341, w1=0.9799926339611957\n",
      "Gradient Descent(174/999): loss=9.77276985996968, w0=-1.7827889687142178, w1=0.9807606876308046\n",
      "Gradient Descent(175/999): loss=9.76687644530554, w0=-1.7904067094876024, w1=0.9815259723197424\n",
      "Gradient Descent(176/999): loss=9.761025447820876, w0=-1.7979969868456152, w1=0.9822884980107156\n",
      "Gradient Descent(177/999): loss=9.755216562222886, w0=-1.8055598997991318, w1=0.9830482746504403\n",
      "Gradient Descent(178/999): loss=9.749449485416067, w0=-1.8130955470020746, w1=0.9838053121497733\n",
      "Gradient Descent(179/999): loss=9.743723916486417, w0=-1.8206040267526997, w1=0.9845596203838407\n",
      "Gradient Descent(180/999): loss=9.738039556685724, w0=-1.8280854369948791, w1=0.9853112091921663\n",
      "Gradient Descent(181/999): loss=9.732396109415994, w0=-1.8355398753193786, w1=0.9860600883788019\n",
      "Gradient Descent(182/999): loss=9.726793280213947, w0=-1.84296743896513, w1=0.9868062677124523\n",
      "Gradient Descent(183/999): loss=9.721230776735673, w0=-1.850368224820501, w1=0.9875497569266051\n",
      "Gradient Descent(184/999): loss=9.715708308741384, w0=-1.8577423294245572, w1=0.9882905657196566\n",
      "Gradient Descent(185/999): loss=9.710225588080256, w0=-1.8650898489683232, w1=0.9890287037550389\n",
      "Gradient Descent(186/999): loss=9.704782328675387, w0=-1.8724108792960363, w1=0.9897641806613445\n",
      "Gradient Descent(187/999): loss=9.699378246508891, w0=-1.8797055159063965, w1=0.9904970060324546\n",
      "Gradient Descent(188/999): loss=9.694013059607071, w0=-1.8869738539538132, w1=0.9912271894276611\n",
      "Gradient Descent(189/999): loss=9.688686488025688, w0=-1.8942159882496457, w1=0.9919547403717935\n",
      "Gradient Descent(190/999): loss=9.68339825383539, w0=-1.9014320132634401, w1=0.9926796683553429\n",
      "Gradient Descent(191/999): loss=9.67814808110717, w0=-1.908622023124162, w1=0.9934019828345841\n",
      "Gradient Descent(192/999): loss=9.672935695898001, w0=-1.9157861116214236, w1=0.9941216932317017\n",
      "Gradient Descent(193/999): loss=9.667760826236542, w0=-1.9229243722067082, w1=0.9948388089349102\n",
      "Gradient Descent(194/999): loss=9.662623202108893, w0=-1.9300368979945879, w1=0.9955533392985791\n",
      "Gradient Descent(195/999): loss=9.657522555444595, w0=-1.9371237817639393, w1=0.9962652936433523\n",
      "Gradient Descent(196/999): loss=9.652458620102568, w0=-1.9441851159591532, w1=0.996974681256272\n",
      "Gradient Descent(197/999): loss=9.647431131857262, w0=-1.9512209926913409, w1=0.9976815113908978\n",
      "Gradient Descent(198/999): loss=9.642439828384862, w0=-1.9582315037395353, w1=0.9983857932674298\n",
      "Gradient Descent(199/999): loss=9.63748444924958, w0=-1.965216740551888, w1=0.9990875360728265\n",
      "Gradient Descent(200/999): loss=9.632564735890115, w0=-1.9721767942468635, w1=0.9997867489609259\n",
      "Gradient Descent(201/999): loss=9.627680431606114, w0=-1.9791117556144258, w1=1.0004834410525645\n",
      "Gradient Descent(202/999): loss=9.62283128154478, w0=-1.9860217151172237, w1=1.0011776214356973\n",
      "Gradient Descent(203/999): loss=9.618017032687634, w0=-1.9929067628917714, w1=1.0018692991655138\n",
      "Gradient Descent(204/999): loss=9.613237433837211, w0=-1.9997669887496232, w1=1.00255848326456\n",
      "Gradient Descent(205/999): loss=9.608492235604075, w0=-2.006602482178546, w1=1.0032451827228517\n",
      "Gradient Descent(206/999): loss=9.603781190393683, w0=-2.013413332343686, w1=1.0039294064979953\n",
      "Gradient Descent(207/999): loss=9.599104052393557, w0=-2.020199628088731, w1=1.0046111635153034\n",
      "Gradient Descent(208/999): loss=9.594460577560435, w0=-2.0269614579370723, w1=1.00529046266791\n",
      "Gradient Descent(209/999): loss=9.589850523607497, w0=-2.0336989100929554, w1=1.0059673128168893\n",
      "Gradient Descent(210/999): loss=9.58527364999178, w0=-2.0404120724426336, w1=1.0066417227913678\n",
      "Gradient Descent(211/999): loss=9.580729717901596, w0=-2.047101032555513, w1=1.0073137013886428\n",
      "Gradient Descent(212/999): loss=9.57621849024407, w0=-2.053765877685296, w1=1.0079832573742948\n",
      "Gradient Descent(213/999): loss=9.571739731632775, w0=-2.0604066947711175, w1=1.0086503994823033\n",
      "Gradient Descent(214/999): loss=9.567293208375466, w0=-2.0670235704386815, w1=1.0093151364151594\n",
      "Gradient Descent(215/999): loss=9.56287868846186, w0=-2.073616591001388, w1=1.0099774768439806\n",
      "Gradient Descent(216/999): loss=9.558495941551536, w0=-2.080185842461463, w1=1.0106374294086231\n",
      "Gradient Descent(217/999): loss=9.55414473896193, w0=-2.0867314105110757, w1=1.0112950027177954\n",
      "Gradient Descent(218/999): loss=9.5498248536564, w0=-2.0932533805334597, w1=1.0119502053491687\n",
      "Gradient Descent(219/999): loss=9.545536060232365, w0=-2.0997518376040256, w1=1.0126030458494917\n",
      "Gradient Descent(220/999): loss=9.541278134909561, w0=-2.106226866491471, w1=1.0132535327346983\n",
      "Gradient Descent(221/999): loss=9.537050855518336, w0=-2.1126785516588855, w1=1.013901674490023\n",
      "Gradient Descent(222/999): loss=9.532854001488117, w0=-2.119106977264854, w1=1.0145474795701084\n",
      "Gradient Descent(223/999): loss=9.528687353835823, w0=-2.1255122271645526, w1=1.0151909563991162\n",
      "Gradient Descent(224/999): loss=9.524550695154506, w0=-2.131894384910844, w1=1.0158321133708377\n",
      "Gradient Descent(225/999): loss=9.52044380960196, w0=-2.1382535337553668, w1=1.0164709588488032\n",
      "Gradient Descent(226/999): loss=9.516366482889495, w0=-2.144589756649621, w1=1.0171075011663908\n",
      "Gradient Descent(227/999): loss=9.512318502270732, w0=-2.150903136246051, w1=1.0177417486269347\n",
      "Gradient Descent(228/999): loss=9.508299656530516, w0=-2.157193754899123, w1=1.0183737095038343\n",
      "Gradient Descent(229/999): loss=9.504309735973875, w0=-2.1634616946664007, w1=1.0190033920406625\n",
      "Gradient Descent(230/999): loss=9.50034853241511, w0=-2.169707037309613, w1=1.0196308044512712\n",
      "Gradient Descent(231/999): loss=9.496415839166886, w0=-2.1759298642957225, w1=1.0202559549199006\n",
      "Gradient Descent(232/999): loss=9.492511451029525, w0=-2.1821302567979886, w1=1.0208788516012852\n",
      "Gradient Descent(233/999): loss=9.488635164280197, w0=-2.1883082956970243, w1=1.02149950262076\n",
      "Gradient Descent(234/999): loss=9.48478677666239, w0=-2.1944640615818534, w1=1.0221179160743663\n",
      "Gradient Descent(235/999): loss=9.480966087375268, w0=-2.200597634750961, w1=1.0227341000289578\n",
      "Gradient Descent(236/999): loss=9.477172897063276, w0=-2.2067090952133395, w1=1.0233480625223061\n",
      "Gradient Descent(237/999): loss=9.47340700780566, w0=-2.212798522689535, w1=1.0239598115632045\n",
      "Gradient Descent(238/999): loss=9.469668223106213, w0=-2.2188659966126854, w1=1.0245693551315733\n",
      "Gradient Descent(239/999): loss=9.46595634788297, w0=-2.224911596129556, w1=1.0251767011785635\n",
      "Gradient Descent(240/999): loss=9.462271188458033, w0=-2.2309354001015738, w1=1.0257818576266609\n",
      "Gradient Descent(241/999): loss=9.458612552547502, w0=-2.236937487105855, w1=1.0263848323697893\n",
      "Gradient Descent(242/999): loss=9.45498024925141, w0=-2.24291793543623, w1=1.026985633273413\n",
      "Gradient Descent(243/999): loss=9.45137408904376, w0=-2.2488768231042657, w1=1.02758426817464\n",
      "Gradient Descent(244/999): loss=9.447793883762674, w0=-2.254814227840281, w1=1.0281807448823241\n",
      "Gradient Descent(245/999): loss=9.444239446600516, w0=-2.2607302270943634, w1=1.028775071177167\n",
      "Gradient Descent(246/999): loss=9.440710592094197, w0=-2.266624898037377, w1=1.0293672548118185\n",
      "Gradient Descent(247/999): loss=9.43720713611547, w0=-2.272498317561971, w1=1.0299573035109792\n",
      "Gradient Descent(248/999): loss=9.433728895861325, w0=-2.2783505622835816, w1=1.0305452249715015\n",
      "Gradient Descent(249/999): loss=9.430275689844462, w0=-2.284181708541431, w1=1.0311310268624874\n",
      "Gradient Descent(250/999): loss=9.42684733788383, w0=-2.289991832399525, w1=1.0317147168253917\n",
      "Gradient Descent(251/999): loss=9.423443661095186, w0=-2.2957810096476434, w1=1.03229630247412\n",
      "Gradient Descent(252/999): loss=9.420064481881786, w0=-2.3015493158023292, w1=1.032875791395128\n",
      "Gradient Descent(253/999): loss=9.416709623925142, w0=-2.307296826107874, w1=1.033453191147521\n",
      "Gradient Descent(254/999): loss=9.413378912175784, w0=-2.3130236155372996, w1=1.0340285092631523\n",
      "Gradient Descent(255/999): loss=9.410072172844139, w0=-2.318729758793335, w1=1.0346017532467213\n",
      "Gradient Descent(256/999): loss=9.406789233391448, w0=-2.3244153303093924, w1=1.0351729305758715\n",
      "Gradient Descent(257/999): loss=9.403529922520828, w0=-2.3300804042505368, w1=1.035742048701288\n",
      "Gradient Descent(258/999): loss=9.400294070168231, w0=-2.3357250545144534, w1=1.0363091150467953\n",
      "Gradient Descent(259/999): loss=9.39708150749367, w0=-2.3413493547324133, w1=1.0368741370094532\n",
      "Gradient Descent(260/999): loss=9.39389206687233, w0=-2.346953378270232, w1=1.037437121959653\n",
      "Gradient Descent(261/999): loss=9.390725581885881, w0=-2.352537198229227, w1=1.0379980772412156\n",
      "Gradient Descent(262/999): loss=9.387581887313758, w0=-2.358100887447172, w1=1.0385570101714854\n",
      "Gradient Descent(263/999): loss=9.384460819124557, w0=-2.3636445184992465, w1=1.0391139280414259\n",
      "Gradient Descent(264/999): loss=9.381362214467462, w0=-2.369168163698982, w1=1.0396688381157164\n",
      "Gradient Descent(265/999): loss=9.378285911663768, w0=-2.374671895099207, w1=1.0402217476328441\n",
      "Gradient Descent(266/999): loss=9.375231750198434, w0=-2.3801557844929846, w1=1.0407726638052024\n",
      "Gradient Descent(267/999): loss=9.3721995707117, w0=-2.385619903414551, w1=1.0413215938191795\n",
      "Gradient Descent(268/999): loss=9.36918921499079, w0=-2.3910643231402466, w1=1.0418685448352585\n",
      "Gradient Descent(269/999): loss=9.366200525961629, w0=-2.396489114689449, w1=1.042413523988105\n",
      "Gradient Descent(270/999): loss=9.363233347680687, w0=-2.4018943488254947, w1=1.0429565383866641\n",
      "Gradient Descent(271/999): loss=9.360287525326791, w0=-2.407280096056607, w1=1.0434975951142522\n",
      "Gradient Descent(272/999): loss=9.357362905193117, w0=-2.4126464266368126, w1=1.0440367012286476\n",
      "Gradient Descent(273/999): loss=9.354459334679088, w0=-2.417993410566859, w1=1.0445738637621849\n",
      "Gradient Descent(274/999): loss=9.351576662282474, w0=-2.4233211175951275, w1=1.0451090897218456\n",
      "Gradient Descent(275/999): loss=9.348714737591461, w0=-2.4286296172185433, w1=1.0456423860893493\n",
      "Gradient Descent(276/999): loss=9.345873411276802, w0=-2.433918978683482, w1=1.0461737598212464\n",
      "Gradient Descent(277/999): loss=9.343052535084043, w0=-2.439189270986673, w1=1.0467032178490057\n",
      "Gradient Descent(278/999): loss=9.340251961825752, w0=-2.444440562876098, w1=1.0472307670791086\n",
      "Gradient Descent(279/999): loss=9.33747154537388, w0=-2.44967292285189, w1=1.0477564143931357\n",
      "Gradient Descent(280/999): loss=9.334711140652118, w0=-2.454886419167227, w1=1.0482801666478594\n",
      "Gradient Descent(281/999): loss=9.331970603628324, w0=-2.4600811198292187, w1=1.0488020306753318\n",
      "Gradient Descent(282/999): loss=9.329249791307005, w0=-2.465257092599798, w1=1.0493220132829735\n",
      "Gradient Descent(283/999): loss=9.326548561721852, w0=-2.4704144049966024, w1=1.0498401212536639\n",
      "Gradient Descent(284/999): loss=9.32386677392838, w0=-2.4755531242938553, w1=1.0503563613458287\n",
      "Gradient Descent(285/999): loss=9.321204287996496, w0=-2.480673317523244, w1=1.0508707402935271\n",
      "Gradient Descent(286/999): loss=9.318560965003263, w0=-2.4857750514747936, w1=1.0513832648065427\n",
      "Gradient Descent(287/999): loss=9.315936667025618, w0=-2.4908583926977386, w1=1.051893941570467\n",
      "Gradient Descent(288/999): loss=9.313331257133175, w0=-2.49592340750139, w1=1.0524027772467899\n",
      "Gradient Descent(289/999): loss=9.310744599381097, w0=-2.500970161956001, w1=1.052909778472985\n",
      "Gradient Descent(290/999): loss=9.308176558802998, w0=-2.5059987218936306, w1=1.0534149518625968\n",
      "Gradient Descent(291/999): loss=9.305627001403895, w0=-2.5110091529089984, w1=1.0539183040053264\n",
      "Gradient Descent(292/999): loss=9.3030957941532, w0=-2.516001520360344, w1=1.0544198414671173\n",
      "Gradient Descent(293/999): loss=9.300582804977834, w0=-2.520975889370277, w1=1.0549195707902432\n",
      "Gradient Descent(294/999): loss=9.298087902755274, w0=-2.525932324826628, w1=1.055417498493389\n",
      "Gradient Descent(295/999): loss=9.295610957306744, w0=-2.5308708913832945, w1=1.0559136310717414\n",
      "Gradient Descent(296/999): loss=9.293151839390399, w0=-2.5357916534610845, w1=1.0564079749970685\n",
      "Gradient Descent(297/999): loss=9.290710420694616, w0=-2.5406946752485564, w1=1.0569005367178075\n",
      "Gradient Descent(298/999): loss=9.288286573831275, w0=-2.545580020702857, w1=1.0573913226591471\n",
      "Gradient Descent(299/999): loss=9.285880172329133, w0=-2.550447753550554, w1=1.057880339223113\n",
      "Gradient Descent(300/999): loss=9.283491090627168, w0=-2.5552979372884703, w1=1.0583675927886493\n",
      "Gradient Descent(301/999): loss=9.281119204068116, w0=-2.5601306351845095, w1=1.0588530897117034\n",
      "Gradient Descent(302/999): loss=9.278764388891885, w0=-2.5649459102784826, w1=1.059336836325308\n",
      "Gradient Descent(303/999): loss=9.276426522229148, w0=-2.56974382538293, w1=1.0598188389396639\n",
      "Gradient Descent(304/999): loss=9.274105482094917, w0=-2.574524443083941, w1=1.060299103842223\n",
      "Gradient Descent(305/999): loss=9.27180114738217, w0=-2.57928782574197, w1=1.0607776372977689\n",
      "Gradient Descent(306/999): loss=9.269513397855548, w0=-2.584034035492649, w1=1.0612544455485005\n",
      "Gradient Descent(307/999): loss=9.267242114145056, w0=-2.5887631342476016, w1=1.0617295348141111\n",
      "Gradient Descent(308/999): loss=9.264987177739872, w0=-2.5934751836952454, w1=1.062202911291872\n",
      "Gradient Descent(309/999): loss=9.262748470982132, w0=-2.598170245301601, w1=1.0626745811567113\n",
      "Gradient Descent(310/999): loss=9.260525877060788, w0=-2.602848380311092, w1=1.0631445505612962\n",
      "Gradient Descent(311/999): loss=9.258319280005546, w0=-2.6075096497473433, w1=1.063612825636111\n",
      "Gradient Descent(312/999): loss=9.256128564680798, w0=-2.6121541144139795, w1=1.0640794124895399\n",
      "Gradient Descent(313/999): loss=9.253953616779587, w0=-2.616781834895415, w1=1.064544317207944\n",
      "Gradient Descent(314/999): loss=9.251794322817684, w0=-2.6213928715576467, w1=1.065007545855743\n",
      "Gradient Descent(315/999): loss=9.249650570127653, w0=-2.6259872845490397, w1=1.0654691044754923\n",
      "Gradient Descent(316/999): loss=9.247522246852961, w0=-2.6305651338011136, w1=1.065928999087963\n",
      "Gradient Descent(317/999): loss=9.245409241942147, w0=-2.6351264790293225, w1=1.0663872356922204\n",
      "Gradient Descent(318/999): loss=9.243311445143046, w0=-2.639671379733836, w1=1.0668438202657016\n",
      "Gradient Descent(319/999): loss=9.241228746996999, w0=-2.6441998952003125, w1=1.0672987587642946\n",
      "Gradient Descent(320/999): loss=9.239161038833172, w0=-2.6487120845006764, w1=1.0677520571224144\n",
      "Gradient Descent(321/999): loss=9.237108212762886, w0=-2.6532080064938848, w1=1.068203721253082\n",
      "Gradient Descent(322/999): loss=9.235070161673974, w0=-2.657687719826697, w1=1.0686537570480004\n",
      "Gradient Descent(323/999): loss=9.233046779225184, w0=-2.662151282934441, w1=1.069102170377632\n",
      "Gradient Descent(324/999): loss=9.231037959840664, w0=-2.666598754041772, w1=1.0695489670912752\n",
      "Gradient Descent(325/999): loss=9.22904359870441, w0=-2.6710301911634367, w1=1.0699941530171406\n",
      "Gradient Descent(326/999): loss=9.22706359175483, w0=-2.6754456521050254, w1=1.0704377339624265\n",
      "Gradient Descent(327/999): loss=9.22509783567932, w0=-2.6798451944637294, w1=1.0708797157133958\n",
      "Gradient Descent(328/999): loss=9.223146227908822, w0=-2.68422887562909, w1=1.07132010403545\n",
      "Gradient Descent(329/999): loss=9.221208666612533, w0=-2.68859675278375, w1=1.0717589046732061\n",
      "Gradient Descent(330/999): loss=9.219285050692552, w0=-2.692948882904196, w1=1.0721961233505706\n",
      "Gradient Descent(331/999): loss=9.217375279778631, w0=-2.6972853227615037, w1=1.072631765770813\n",
      "Gradient Descent(332/999): loss=9.215479254222915, w0=-2.701606128922079, w1=1.073065837616643\n",
      "Gradient Descent(333/999): loss=9.213596875094769, w0=-2.7059113577483953, w1=1.0734983445502821\n",
      "Gradient Descent(334/999): loss=9.211728044175565, w0=-2.7102010653997275, w1=1.0739292922135388\n",
      "Gradient Descent(335/999): loss=9.209872663953641, w0=-2.7144753078328856, w1=1.0743586862278813\n",
      "Gradient Descent(336/999): loss=9.208030637619107, w0=-2.7187341408029453, w1=1.074786532194512\n",
      "Gradient Descent(337/999): loss=9.206201869058921, w0=-2.722977619863974, w1=1.0752128356944395\n",
      "Gradient Descent(338/999): loss=9.204386262851731, w0=-2.7272058003697563, w1=1.0756376022885519\n",
      "Gradient Descent(339/999): loss=9.20258372426302, w0=-2.731418737474516, w1=1.076060837517689\n",
      "Gradient Descent(340/999): loss=9.200794159240099, w0=-2.7356164861336345, w1=1.0764825469027153\n",
      "Gradient Descent(341/999): loss=9.199017474407196, w0=-2.7397991011043694, w1=1.0769027359445908\n",
      "Gradient Descent(342/999): loss=9.197253577060627, w0=-2.743966636946568, w1=1.0773214101244444\n",
      "Gradient Descent(343/999): loss=9.195502375163903, w0=-2.7481191480233775, w1=1.077738574903644\n",
      "Gradient Descent(344/999): loss=9.193763777342987, w0=-2.7522566885019573, w1=1.0781542357238678\n",
      "Gradient Descent(345/999): loss=9.192037692881467, w0=-2.7563793123541824, w1=1.0785683980071765\n",
      "Gradient Descent(346/999): loss=9.190324031715885, w0=-2.7604870733573503, w1=1.0789810671560827\n",
      "Gradient Descent(347/999): loss=9.188622704430976, w0=-2.7645800250948795, w1=1.079392248553622\n",
      "Gradient Descent(348/999): loss=9.186933622255035, w0=-2.768658220957011, w1=1.0798019475634235\n",
      "Gradient Descent(349/999): loss=9.185256697055298, w0=-2.7727217141415035, w1=1.0802101695297788\n",
      "Gradient Descent(350/999): loss=9.18359184133331, w0=-2.7767705576543276, w1=1.0806169197777125\n",
      "Gradient Descent(351/999): loss=9.181938968220397, w0=-2.780804804310357, w1=1.081022203613052\n",
      "Gradient Descent(352/999): loss=9.18029799147307, w0=-2.7848245067340573, w1=1.081426026322495\n",
      "Gradient Descent(353/999): loss=9.17866882546862, w0=-2.788829717360174, w1=1.0818283931736814\n",
      "Gradient Descent(354/999): loss=9.177051385200558, w0=-2.792820488434415, w1=1.0822293094152582\n",
      "Gradient Descent(355/999): loss=9.175445586274254, w0=-2.796796872014131, w1=1.0826287802769516\n",
      "Gradient Descent(356/999): loss=9.17385134490245, w0=-2.800758919968998, w1=1.083026810969633\n",
      "Gradient Descent(357/999): loss=9.172268577900988, w0=-2.8047066839816903, w1=1.0834234066853876\n",
      "Gradient Descent(358/999): loss=9.17069720268439, w0=-2.8086402155485573, w1=1.0838185725975824\n",
      "Gradient Descent(359/999): loss=9.16913713726159, w0=-2.812559565980293, w1=1.084212313860933\n",
      "Gradient Descent(360/999): loss=9.167588300231632, w0=-2.8164647864026082, w1=1.0846046356115715\n",
      "Gradient Descent(361/999): loss=9.166050610779441, w0=-2.820355927756894, w1=1.0849955429671136\n",
      "Gradient Descent(362/999): loss=9.1645239886716, w0=-2.8242330408008893, w1=1.0853850410267243\n",
      "Gradient Descent(363/999): loss=9.16300835425216, w0=-2.828096176109341, w1=1.0857731348711857\n",
      "Gradient Descent(364/999): loss=9.161503628438485, w0=-2.8319453840746642, w1=1.0861598295629626\n",
      "Gradient Descent(365/999): loss=9.160009732717134, w0=-2.8357807149076004, w1=1.0865451301462683\n",
      "Gradient Descent(366/999): loss=9.158526589139747, w0=-2.839602218637871, w1=1.0869290416471316\n",
      "Gradient Descent(367/999): loss=9.157054120319, w0=-2.843409945114831, w1=1.0873115690734598\n",
      "Gradient Descent(368/999): loss=9.15559224942456, w0=-2.847203944008119, w1=1.087692717415108\n",
      "Gradient Descent(369/999): loss=9.154140900179042, w0=-2.850984264808305, w1=1.0880724916439404\n",
      "Gradient Descent(370/999): loss=9.152699996854086, w0=-2.8547509568275355, w1=1.0884508967138966\n",
      "Gradient Descent(371/999): loss=9.151269464266388, w0=-2.858504069200178, w1=1.0888279375610572\n",
      "Gradient Descent(372/999): loss=9.149849227773718, w0=-2.8622436508834608, w1=1.0892036191037067\n",
      "Gradient Descent(373/999): loss=9.148439213271136, w0=-2.8659697506581123, w1=1.0895779462423985\n",
      "Gradient Descent(374/999): loss=9.147039347187016, w0=-2.8696824171289967, w1=1.089950923860018\n",
      "Gradient Descent(375/999): loss=9.145649556479283, w0=-2.8733816987257486, w1=1.0903225568218478\n",
      "Gradient Descent(376/999): loss=9.144269768631569, w0=-2.877067643703404, w1=1.0906928499756294\n",
      "Gradient Descent(377/999): loss=9.14289991164942, w0=-2.880740300143031, w1=1.0910618081516277\n",
      "Gradient Descent(378/999): loss=9.141539914056573, w0=-2.884399715952356, w1=1.0914294361626933\n",
      "Gradient Descent(379/999): loss=9.140189704891194, w0=-2.8880459388663877, w1=1.091795738804326\n",
      "Gradient Descent(380/999): loss=9.13884921370219, w0=-2.891679016448043, w1=1.0921607208547364\n",
      "Gradient Descent(381/999): loss=9.13751837054555, w0=-2.895298996088764, w1=1.0925243870749088\n",
      "Gradient Descent(382/999): loss=9.136197105980653, w0=-2.8989059250091374, w1=1.0928867422086637\n",
      "Gradient Descent(383/999): loss=9.134885351066663, w0=-2.902499850259512, w1=1.0932477909827187\n",
      "Gradient Descent(384/999): loss=9.133583037358976, w0=-2.9060808187206097, w1=1.0936075381067507\n",
      "Gradient Descent(385/999): loss=9.13229009690558, w0=-2.909648877104139, w1=1.093965988273458\n",
      "Gradient Descent(386/999): loss=9.13100646224354, w0=-2.9132040719534036, w1=1.0943231461586196\n",
      "Gradient Descent(387/999): loss=9.129732066395492, w0=-2.91674644964391, w1=1.094679016421159\n",
      "Gradient Descent(388/999): loss=9.128466842866123, w0=-2.9202760563839716, w1=1.0950336037032018\n",
      "Gradient Descent(389/999): loss=9.12721072563872, w0=-2.923792938215312, w1=1.0953869126301388\n",
      "Gradient Descent(390/999): loss=9.125963649171705, w0=-2.927297141013666, w1=1.0957389478106851\n",
      "Gradient Descent(391/999): loss=9.124725548395249, w0=-2.9307887104893773, w1=1.096089713836941\n",
      "Gradient Descent(392/999): loss=9.12349635870784, w0=-2.934267692187995, w1=1.0964392152844498\n",
      "Gradient Descent(393/999): loss=9.122276015972917, w0=-2.9377341314908683, w1=1.0967874567122615\n",
      "Gradient Descent(394/999): loss=9.12106445651557, w0=-2.9411880736157374, w1=1.0971344426629877\n",
      "Gradient Descent(395/999): loss=9.119861617119156, w0=-2.944629563617324, w1=1.0974801776628647\n",
      "Gradient Descent(396/999): loss=9.118667435022022, w0=-2.9480586463879184, w1=1.097824666221809\n",
      "Gradient Descent(397/999): loss=9.11748184791425, w0=-2.9514753666579665, w1=1.09816791283348\n",
      "Gradient Descent(398/999): loss=9.116304793934384, w0=-2.9548797689966517, w1=1.0985099219753351\n",
      "Gradient Descent(399/999): loss=9.115136211666217, w0=-2.9582718978124776, w1=1.0988506981086905\n",
      "Gradient Descent(400/999): loss=9.113976040135562, w0=-2.961651797353846, w1=1.0991902456787777\n",
      "Gradient Descent(401/999): loss=9.112824218807098, w0=-2.965019511709635, w1=1.0995285691148031\n",
      "Gradient Descent(402/999): loss=9.111680687581188, w0=-2.968375084809774, w1=1.0998656728300042\n",
      "Gradient Descent(403/999): loss=9.110545386790792, w0=-2.971718560425816, w1=1.1002015612217089\n",
      "Gradient Descent(404/999): loss=9.109418257198266, w0=-2.9750499821715097, w1=1.1005362386713908\n",
      "Gradient Descent(405/999): loss=9.108299239992352, w0=-2.978369393503368, w1=1.1008697095447282\n",
      "Gradient Descent(406/999): loss=9.107188276785072, w0=-2.9816768377212344, w1=1.10120197819166\n",
      "Gradient Descent(407/999): loss=9.106085309608693, w0=-2.984972357968848, w1=1.1015330489464426\n",
      "Gradient Descent(408/999): loss=9.104990280912693, w0=-2.9882559972344067, w1=1.1018629261277073\n",
      "Gradient Descent(409/999): loss=9.103903133560763, w0=-2.991527798351128, w1=1.1021916140385146\n",
      "Gradient Descent(410/999): loss=9.10282381082784, w0=-2.994787803997807, w1=1.1025191169664132\n",
      "Gradient Descent(411/999): loss=9.101752256397107, w0=-2.998036056699374, w1=1.1028454391834928\n",
      "Gradient Descent(412/999): loss=9.100688414357098, w0=-3.001272598827448, w1=1.103170584946443\n",
      "Gradient Descent(413/999): loss=9.099632229198775, w0=-3.0044974726008906, w1=1.1034945584966056\n",
      "Gradient Descent(414/999): loss=9.098583645812583, w0=-3.007710720086357, w1=1.103817364060033\n",
      "Gradient Descent(415/999): loss=9.097542609485656, w0=-3.010912383198843, w1=1.1041390058475409\n",
      "Gradient Descent(416/999): loss=9.096509065898887, w0=-3.0141025037022335, w1=1.1044594880547645\n",
      "Gradient Descent(417/999): loss=9.095482961124135, w0=-3.0172811232098464, w1=1.104778814862213\n",
      "Gradient Descent(418/999): loss=9.094464241621397, w0=-3.0204482831849755, w1=1.105096990435324\n",
      "Gradient Descent(419/999): loss=9.09345285423602, w0=-3.0236040249414313, w1=1.1054140189245183\n",
      "Gradient Descent(420/999): loss=9.092448746195926, w0=-3.0267483896440806, w1=1.1057299044652524\n",
      "Gradient Descent(421/999): loss=9.091451865108864, w0=-3.0298814183093823, w1=1.1060446511780753\n",
      "Gradient Descent(422/999): loss=9.09046215895966, w0=-3.033003151805924, w1=1.1063582631686795\n",
      "Gradient Descent(423/999): loss=9.089479576107516, w0=-3.0361136308549534, w1=1.106670744527956\n",
      "Gradient Descent(424/999): loss=9.088504065283326, w0=-3.039212896030911, w1=1.1069820993320478\n",
      "Gradient Descent(425/999): loss=9.087535575586953, w0=-3.0423009877619576, w1=1.107292331642402\n",
      "Gradient Descent(426/999): loss=9.086574056484652, w0=-3.045377946330504, w1=1.1076014455058236\n",
      "Gradient Descent(427/999): loss=9.085619457806358, w0=-3.0484438118737343, w1=1.1079094449545281\n",
      "Gradient Descent(428/999): loss=9.084671729743107, w0=-3.051498624384131, w1=1.1082163340061943\n",
      "Gradient Descent(429/999): loss=9.083730822844444, w0=-3.0545424237099956, w1=1.108522116664016\n",
      "Gradient Descent(430/999): loss=9.082796688015794, w0=-3.0575752495559687, w1=1.1088267969167556\n",
      "Gradient Descent(431/999): loss=9.081869276515976, w0=-3.0605971414835484, w1=1.1091303787387943\n",
      "Gradient Descent(432/999): loss=9.08094853995459, w0=-3.063608138911606, w1=1.109432866090185\n",
      "Gradient Descent(433/999): loss=9.08003443028953, w0=-3.0666082811169, w1=1.1097342629167048\n",
      "Gradient Descent(434/999): loss=9.079126899824466, w0=-3.0695976072345887, w1=1.1100345731499044\n",
      "Gradient Descent(435/999): loss=9.078225901206345, w0=-3.072576156258741, w1=1.1103338007071613\n",
      "Gradient Descent(436/999): loss=9.07733138742296, w0=-3.075543967042844, w1=1.1106319494917292\n",
      "Gradient Descent(437/999): loss=9.07644331180046, w0=-3.0785010783003117, w1=1.1109290233927909\n",
      "Gradient Descent(438/999): loss=9.07556162800089, w0=-3.0814475286049876, w1=1.1112250262855072\n",
      "Gradient Descent(439/999): loss=9.074686290019851, w0=-3.08438335639165, w1=1.1115199620310685\n",
      "Gradient Descent(440/999): loss=9.073817252184039, w0=-3.0873085999565113, w1=1.1118138344767448\n",
      "Gradient Descent(441/999): loss=9.072954469148868, w0=-3.09022329745772, w1=1.112106647455936\n",
      "Gradient Descent(442/999): loss=9.072097895896137, w0=-3.093127486915857, w1=1.1123984047882218\n",
      "Gradient Descent(443/999): loss=9.071247487731638, w0=-3.0960212062144308, w1=1.1126891102794114\n",
      "Gradient Descent(444/999): loss=9.070403200282847, w0=-3.098904493100373, w1=1.112978767721594\n",
      "Gradient Descent(445/999): loss=9.06956498949663, w0=-3.101777385184531, w1=1.1132673808931866\n",
      "Gradient Descent(446/999): loss=9.068732811636872, w0=-3.1046399199421573, w1=1.113554953558985\n",
      "Gradient Descent(447/999): loss=9.067906623282301, w0=-3.1074921347133984, w1=1.1138414894702122\n",
      "Gradient Descent(448/999): loss=9.067086381324124, w0=-3.1103340667037833, w1=1.1141269923645667\n",
      "Gradient Descent(449/999): loss=9.066272042963833, w0=-3.1131657529847074, w1=1.1144114659662727\n",
      "Gradient Descent(450/999): loss=9.065463565710957, w0=-3.1159872304939173, w1=1.114694913986127\n",
      "Gradient Descent(451/999): loss=9.064660907380844, w0=-3.118798536035991, w1=1.1149773401215495\n",
      "Gradient Descent(452/999): loss=9.06386402609246, w0=-3.12159970628282, w1=1.1152587480566285\n",
      "Gradient Descent(453/999): loss=9.063072880266215, w0=-3.124390777774085, w1=1.115539141462172\n",
      "Gradient Descent(454/999): loss=9.062287428621762, w0=-3.127171786917736, w1=1.1158185239957532\n",
      "Gradient Descent(455/999): loss=9.06150763017588, w0=-3.1299427699904645, w1=1.116096899301759\n",
      "Gradient Descent(456/999): loss=9.060733444240324, w0=-3.1327037631381773, w1=1.1163742710114384\n",
      "Gradient Descent(457/999): loss=9.059964830419688, w0=-3.1354548023764686, w1=1.1166506427429481\n",
      "Gradient Descent(458/999): loss=9.059201748609313, w0=-3.1381959235910895, w1=1.116926018101401\n",
      "Gradient Descent(459/999): loss=9.058444158993183, w0=-3.140927162538416, w1=1.1172004006789131\n",
      "Gradient Descent(460/999): loss=9.057692022041849, w0=-3.1436485548459157, w1=1.1174737940546497\n",
      "Gradient Descent(461/999): loss=9.0569452985104, w0=-3.1463601360126123, w1=1.1177462017948732\n",
      "Gradient Descent(462/999): loss=9.056203949436345, w0=-3.149061941409548, w1=1.1180176274529883\n",
      "Gradient Descent(463/999): loss=9.055467936137648, w0=-3.1517540062802474, w1=1.1182880745695887\n",
      "Gradient Descent(464/999): loss=9.054737220210674, w0=-3.1544363657411734, w1=1.118557546672505\n",
      "Gradient Descent(465/999): loss=9.054011763528187, w0=-3.1571090547821883, w1=1.1188260472768479\n",
      "Gradient Descent(466/999): loss=9.053291528237386, w0=-3.159772108267009, w1=1.1190935798850559\n",
      "Gradient Descent(467/999): loss=9.052576476757904, w0=-3.1624255609336624, w1=1.1193601479869408\n",
      "Gradient Descent(468/999): loss=9.051866571779833, w0=-3.1650694473949383, w1=1.1196257550597322\n",
      "Gradient Descent(469/999): loss=9.051161776261829, w0=-3.1677038021388397, w1=1.1198904045681253\n",
      "Gradient Descent(470/999): loss=9.05046205342913, w0=-3.170328659529035, w1=1.1201540999643216\n",
      "Gradient Descent(471/999): loss=9.049767366771674, w0=-3.172944053805304, w1=1.1204168446880802\n",
      "Gradient Descent(472/999): loss=9.04907768004215, w0=-3.175550019083986, w1=1.1206786421667558\n",
      "Gradient Descent(473/999): loss=9.048392957254155, w0=-3.178146589358424, w1=1.1209394958153498\n",
      "Gradient Descent(474/999): loss=9.047713162680289, w0=-3.1807337984994097, w1=1.1211994090365496\n",
      "Gradient Descent(475/999): loss=9.047038260850277, w0=-3.1833116802556223, w1=1.121458385220777\n",
      "Gradient Descent(476/999): loss=9.04636821654918, w0=-3.185880268254072, w1=1.1217164277462286\n",
      "Gradient Descent(477/999): loss=9.04570299481547, w0=-3.1884395960005363, w1=1.1219735399789248\n",
      "Gradient Descent(478/999): loss=9.045042560939272, w0=-3.1909896968799982, w1=1.1222297252727478\n",
      "Gradient Descent(479/999): loss=9.044386880460523, w0=-3.1935306041570817, w1=1.122484986969491\n",
      "Gradient Descent(480/999): loss=9.043735919167197, w0=-3.196062350976485, w1=1.1227393283988982\n",
      "Gradient Descent(481/999): loss=9.04308964309349, w0=-3.1985849703634144, w1=1.12299275287871\n",
      "Gradient Descent(482/999): loss=9.04244801851807, w0=-3.201098495224012, w1=1.1232452637147048\n",
      "Gradient Descent(483/999): loss=9.041811011962306, w0=-3.203602958345789, w1=1.1234968642007441\n",
      "Gradient Descent(484/999): loss=9.04117859018853, w0=-3.20609839239805, w1=1.123747557618814\n",
      "Gradient Descent(485/999): loss=9.0405507201983, w0=-3.208584829932321, w1=1.1239973472390683\n",
      "Gradient Descent(486/999): loss=9.039927369230675, w0=-3.211062303382774, w1=1.124246236319871\n",
      "Gradient Descent(487/999): loss=9.039308504760498, w0=-3.2135308450666487, w1=1.1244942281078398\n",
      "Gradient Descent(488/999): loss=9.03869409449672, w0=-3.215990487184675, w1=1.124741325837887\n",
      "Gradient Descent(489/999): loss=9.038084106380698, w0=-3.2184412618214933, w1=1.1249875327332632\n",
      "Gradient Descent(490/999): loss=9.037478508584533, w0=-3.2208832009460733, w1=1.1252328520055976\n",
      "Gradient Descent(491/999): loss=9.03687726950939, w0=-3.2233163364121293, w1=1.1254772868549419\n",
      "Gradient Descent(492/999): loss=9.036280357783877, w0=-3.225740699958538, w1=1.1257208404698098\n",
      "Gradient Descent(493/999): loss=9.035687742262379, w0=-3.2281563232097503, w1=1.1259635160272212\n",
      "Gradient Descent(494/999): loss=9.03509939202348, w0=-3.230563237676206, w1=1.1262053166927408\n",
      "Gradient Descent(495/999): loss=9.03451527636827, w0=-3.2329614747547426, w1=1.1264462456205222\n",
      "Gradient Descent(496/999): loss=9.033935364818834, w0=-3.2353510657290068, w1=1.1266863059533465\n",
      "Gradient Descent(497/999): loss=9.033359627116598, w0=-3.237732041769861, w1=1.1269255008226653\n",
      "Gradient Descent(498/999): loss=9.032788033220763, w0=-3.240104433935792, w1=1.1271638333486402\n",
      "Gradient Descent(499/999): loss=9.032220553306768, w0=-3.242468273173313, w1=1.1274013066401845\n",
      "Gradient Descent(500/999): loss=9.03165715776469, w0=-3.2448235903173703, w1=1.1276379237950025\n",
      "Gradient Descent(501/999): loss=9.031097817197734, w0=-3.2471704160917443, w1=1.1278736878996316\n",
      "Gradient Descent(502/999): loss=9.03054250242067, w0=-3.2495087811094496, w1=1.1281086020294808\n",
      "Gradient Descent(503/999): loss=9.029991184458341, w0=-3.251838715873136, w1=1.1283426692488727\n",
      "Gradient Descent(504/999): loss=9.029443834544127, w0=-3.2541602507754845, w1=1.1285758926110814\n",
      "Gradient Descent(505/999): loss=9.028900424118463, w0=-3.256473416099605, w1=1.1288082751583737\n",
      "Gradient Descent(506/999): loss=9.02836092482732, w0=-3.258778242019431, w1=1.1290398199220486\n",
      "Gradient Descent(507/999): loss=9.027825308520754, w0=-3.261074758600113, w1=1.1292705299224766\n",
      "Gradient Descent(508/999): loss=9.02729354725143, w0=-3.2633629957984116, w1=1.129500408169139\n",
      "Gradient Descent(509/999): loss=9.026765613273165, w0=-3.2656429834630862, w1=1.1297294576606682\n",
      "Gradient Descent(510/999): loss=9.026241479039449, w0=-3.267914751335287, w1=1.1299576813848844\n",
      "Gradient Descent(511/999): loss=9.025721117202064, w0=-3.270178329048941, w1=1.1301850823188375\n",
      "Gradient Descent(512/999): loss=9.025204500609627, w0=-3.2724337461311395, w1=1.1304116634288441\n",
      "Gradient Descent(513/999): loss=9.024691602306152, w0=-3.2746810320025226, w1=1.1306374276705264\n",
      "Gradient Descent(514/999): loss=9.024182395529685, w0=-3.2769202159776634, w1=1.1308623779888516\n",
      "Gradient Descent(515/999): loss=9.023676853710878, w0=-3.279151327265451, w1=1.1310865173181688\n",
      "Gradient Descent(516/999): loss=9.023174950471624, w0=-3.28137439496947, w1=1.1313098485822495\n",
      "Gradient Descent(517/999): loss=9.022676659623652, w0=-3.2835894480883816, w1=1.1315323746943233\n",
      "Gradient Descent(518/999): loss=9.0221819551672, w0=-3.285796515516301, w1=1.1317540985571175\n",
      "Gradient Descent(519/999): loss=9.021690811289607, w0=-3.2879956260431746, w1=1.1319750230628949\n",
      "Gradient Descent(520/999): loss=9.021203202364028, w0=-3.2901868083551555, w1=1.1321951510934898\n",
      "Gradient Descent(521/999): loss=9.020719102948037, w0=-3.292370091034978, w1=1.1324144855203493\n",
      "Gradient Descent(522/999): loss=9.02023848778236, w0=-3.2945455025623294, w1=1.1326330292045654\n",
      "Gradient Descent(523/999): loss=9.019761331789482, w0=-3.2967130713142234, w1=1.1328507849969183\n",
      "Gradient Descent(524/999): loss=9.019287610072409, w0=-3.2988728255653683, w1=1.1330677557379079\n",
      "Gradient Descent(525/999): loss=9.01881729791333, w0=-3.3010247934885366, w1=1.1332839442577958\n",
      "Gradient Descent(526/999): loss=9.018350370772348, w0=-3.3031690031549332, w1=1.1334993533766382\n",
      "Gradient Descent(527/999): loss=9.01788680428617, w0=-3.3053054825345605, w1=1.133713985904326\n",
      "Gradient Descent(528/999): loss=9.017426574266887, w0=-3.307434259496584, w1=1.1339278446406182\n",
      "Gradient Descent(529/999): loss=9.016969656700633, w0=-3.3095553618096947, w1=1.134140932375182\n",
      "Gradient Descent(530/999): loss=9.016516027746421, w0=-3.3116688171424733, w1=1.134353251887626\n",
      "Gradient Descent(531/999): loss=9.01606566373484, w0=-3.313774653063749, w1=1.1345648059475384\n",
      "Gradient Descent(532/999): loss=9.015618541166832, w0=-3.3158728970429605, w1=1.1347755973145217\n",
      "Gradient Descent(533/999): loss=9.015174636712489, w0=-3.317963576450514, w1=1.1349856287382305\n",
      "Gradient Descent(534/999): loss=9.01473392720978, w0=-3.32004671855814, w1=1.1351949029584059\n",
      "Gradient Descent(535/999): loss=9.014296389663443, w0=-3.3221223505392494, w1=1.1354034227049117\n",
      "Gradient Descent(536/999): loss=9.013862001243652, w0=-3.3241904994692875, w1=1.13561119069777\n",
      "Gradient Descent(537/999): loss=9.013430739284939, w0=-3.326251192326087, w1=1.135818209647197\n",
      "Gradient Descent(538/999): loss=9.013002581284958, w0=-3.3283044559902213, w1=1.1360244822536374\n",
      "Gradient Descent(539/999): loss=9.01257750490333, w0=-3.3303503172453537, w1=1.1362300112078012\n",
      "Gradient Descent(540/999): loss=9.012155487960436, w0=-3.332388802778587, w1=1.136434799190697\n",
      "Gradient Descent(541/999): loss=9.011736508436332, w0=-3.3344199391808123, w1=1.1366388488736685\n",
      "Gradient Descent(542/999): loss=9.011320544469541, w0=-3.3364437529470554, w1=1.136842162918428\n",
      "Gradient Descent(543/999): loss=9.010907574355928, w0=-3.3384602704768223, w1=1.137044743977093\n",
      "Gradient Descent(544/999): loss=9.010497576547571, w0=-3.3404695180744435, w1=1.1372465946922177\n",
      "Gradient Descent(545/999): loss=9.010090529651656, w0=-3.342471521949418, w1=1.1374477176968318\n",
      "Gradient Descent(546/999): loss=9.009686412429328, w0=-3.344466308216754, w1=1.13764811561447\n",
      "Gradient Descent(547/999): loss=9.009285203794587, w0=-3.34645390289731, w1=1.1378477910592109\n",
      "Gradient Descent(548/999): loss=9.008886882813206, w0=-3.348434331918135, w1=1.1380467466357065\n",
      "Gradient Descent(549/999): loss=9.008491428701626, w0=-3.3504076211128053, w1=1.138244984939221\n",
      "Gradient Descent(550/999): loss=9.008098820825891, w0=-3.3523737962217623, w1=1.1384425085556602\n",
      "Gradient Descent(551/999): loss=9.007709038700527, w0=-3.3543328828926486, w1=1.1386393200616085\n",
      "Gradient Descent(552/999): loss=9.007322061987539, w0=-3.356284906680642, w1=1.13883542202436\n",
      "Gradient Descent(553/999): loss=9.006937870495282, w0=-3.358229893048789, w1=1.1390308170019545\n",
      "Gradient Descent(554/999): loss=9.006556444177436, w0=-3.3601678673683364, w1=1.1392255075432085\n",
      "Gradient Descent(555/999): loss=9.006177763131987, w0=-3.362098854919063, w1=1.1394194961877506\n",
      "Gradient Descent(556/999): loss=9.00580180760015, w0=-3.36402288088961, w1=1.1396127854660525\n",
      "Gradient Descent(557/999): loss=9.005428557965345, w0=-3.365939970377808, w1=1.1398053778994637\n",
      "Gradient Descent(558/999): loss=9.005057994752196, w0=-3.3678501483910046, w1=1.1399972760002435\n",
      "Gradient Descent(559/999): loss=9.00469009862548, w0=-3.3697534398463924, w1=1.1401884822715942\n",
      "Gradient Descent(560/999): loss=9.004324850389152, w0=-3.371649869571332, w1=1.1403789992076934\n",
      "Gradient Descent(561/999): loss=9.003962230985323, w0=-3.3735394623036763, w1=1.1405688292937266\n",
      "Gradient Descent(562/999): loss=9.003602221493265, w0=-3.375422242692094, w1=1.1407579750059202\n",
      "Gradient Descent(563/999): loss=9.003244803128446, w0=-3.3772982352963905, w1=1.1409464388115733\n",
      "Gradient Descent(564/999): loss=9.002889957241523, w0=-3.3791674645878285, w1=1.1411342231690889\n",
      "Gradient Descent(565/999): loss=9.002537665317371, w0=-3.3810299549494465, w1=1.1413213305280085\n",
      "Gradient Descent(566/999): loss=9.002187908974141, w0=-3.3828857306763784, w1=1.1415077633290411\n",
      "Gradient Descent(567/999): loss=9.001840669962277, w0=-3.384734815976169, w1=1.141693524004098\n",
      "Gradient Descent(568/999): loss=9.001495930163562, w0=-3.386577234969091, w1=1.141878614976321\n",
      "Gradient Descent(569/999): loss=9.001153671590219, w0=-3.3884130116884568, w1=1.1420630386601185\n",
      "Gradient Descent(570/999): loss=9.000813876383898, w0=-3.3902421700809366, w1=1.1422467974611918\n",
      "Gradient Descent(571/999): loss=9.000476526814793, w0=-3.392064734006867, w1=1.1424298937765718\n",
      "Gradient Descent(572/999): loss=9.000141605280715, w0=-3.393880727240563, w1=1.1426123299946456\n",
      "Gradient Descent(573/999): loss=8.999809094306155, w0=-3.3956901734706304, w1=1.1427941084951905\n",
      "Gradient Descent(574/999): loss=8.999478976541392, w0=-3.397493096300271, w1=1.1429752316494046\n",
      "Gradient Descent(575/999): loss=8.999151234761548, w0=-3.3992895192475943, w1=1.143155701819937\n",
      "Gradient Descent(576/999): loss=8.99882585186579, w0=-3.401079465745921, w1=1.1433355213609184\n",
      "Gradient Descent(577/999): loss=8.998502810876277, w0=-3.4028629591440915, w1=1.143514692617994\n",
      "Gradient Descent(578/999): loss=8.998182094937414, w0=-3.404640022706768, w1=1.1436932179283508\n",
      "Gradient Descent(579/999): loss=8.997863687314931, w0=-3.40641067961474, w1=1.1438710996207506\n",
      "Gradient Descent(580/999): loss=8.997547571394973, w0=-3.4081749529652257, w1=1.1440483400155597\n",
      "Gradient Descent(581/999): loss=8.997233730683265, w0=-3.4099328657721726, w1=1.1442249414247787\n",
      "Gradient Descent(582/999): loss=8.996922148804273, w0=-3.4116844409665594, w1=1.144400906152073\n",
      "Gradient Descent(583/999): loss=8.996612809500288, w0=-3.4134297013966943, w1=1.1445762364928032\n",
      "Gradient Descent(584/999): loss=8.99630569663065, w0=-3.415168669828512, w1=1.144750934734054\n",
      "Gradient Descent(585/999): loss=8.996000794170838, w0=-3.4169013689458727, w1=1.144925003154666\n",
      "Gradient Descent(586/999): loss=8.995698086211693, w0=-3.4186278213508565, w1=1.1450984440252623\n",
      "Gradient Descent(587/999): loss=8.995397556958526, w0=-3.4203480495640584, w1=1.1452712596082817\n",
      "Gradient Descent(588/999): loss=8.995099190730368, w0=-3.4220620760248823, w1=1.1454434521580052\n",
      "Gradient Descent(589/999): loss=8.994802971959084, w0=-3.4237699230918346, w1=1.145615023920588\n",
      "Gradient Descent(590/999): loss=8.994508885188605, w0=-3.4254716130428142, w1=1.1457859771340857\n",
      "Gradient Descent(591/999): loss=8.994216915074087, w0=-3.4271671680754046, w1=1.1459563140284872\n",
      "Gradient Descent(592/999): loss=8.993927046381154, w0=-3.428856610307162, w1=1.1461260368257402\n",
      "Gradient Descent(593/999): loss=8.993639263985065, w0=-3.4305399617759043, w1=1.1462951477397836\n",
      "Gradient Descent(594/999): loss=8.993353552869918, w0=-3.43221724444, w1=1.1464636489765723\n",
      "Gradient Descent(595/999): loss=8.993069898127924, w0=-3.433888480178653, w1=1.146631542734111\n",
      "Gradient Descent(596/999): loss=8.992788284958564, w0=-3.435553690792188, w1=1.1467988312024775\n",
      "Gradient Descent(597/999): loss=8.992508698667844, w0=-3.4372128980023358, w1=1.1469655165638566\n",
      "Gradient Descent(598/999): loss=8.992231124667553, w0=-3.438866123452516, w1=1.1471316009925634\n",
      "Gradient Descent(599/999): loss=8.991955548474449, w0=-3.4405133887081205, w1=1.147297086655076\n",
      "Gradient Descent(600/999): loss=8.991681955709545, w0=-3.442154715256792, w1=1.1474619757100615\n",
      "Gradient Descent(601/999): loss=8.991410332097345, w0=-3.4437901245087073, w1=1.1476262703084035\n",
      "Gradient Descent(602/999): loss=8.991140663465098, w0=-3.4454196377968556, w1=1.147789972593233\n",
      "Gradient Descent(603/999): loss=8.99087293574205, w0=-3.447043276377316, w1=1.147953084699953\n",
      "Gradient Descent(604/999): loss=8.99060713495874, w0=-3.448661061429535, w1=1.1481156087562694\n",
      "Gradient Descent(605/999): loss=8.99034324724624, w0=-3.450273014056605, w1=1.148277546882216\n",
      "Gradient Descent(606/999): loss=8.990081258835435, w0=-3.451879155285535, w1=1.1484389011901845\n",
      "Gradient Descent(607/999): loss=8.989821156056307, w0=-3.45347950606753, w1=1.1485996737849502\n",
      "Gradient Descent(608/999): loss=8.98956292533725, w0=-3.45507408727826, w1=1.1487598667637011\n",
      "Gradient Descent(609/999): loss=8.989306553204324, w0=-3.456662919718136, w1=1.1489194822160635\n",
      "Gradient Descent(610/999): loss=8.989052026280573, w0=-3.458246024112578, w1=1.1490785222241313\n",
      "Gradient Descent(611/999): loss=8.988799331285303, w0=-3.4598234211122882, w1=1.1492369888624911\n",
      "Gradient Descent(612/999): loss=8.98854845503343, w0=-3.461395131293518, w1=1.1493948841982509\n",
      "Gradient Descent(613/999): loss=8.98829938443475, w0=-3.4629611751583376, w1=1.1495522102910654\n",
      "Gradient Descent(614/999): loss=8.988052106493274, w0=-3.4645215731349035, w1=1.149708969193165\n",
      "Gradient Descent(615/999): loss=8.987806608306569, w0=-3.4660763455777253, w1=1.1498651629493812\n",
      "Gradient Descent(616/999): loss=8.987562877065038, w0=-3.4676255127679303, w1=1.1500207935971725\n",
      "Gradient Descent(617/999): loss=8.987320900051307, w0=-3.469169094913528, w1=1.150175863166653\n",
      "Gradient Descent(618/999): loss=8.987080664639516, w0=-3.470707112149675, w1=1.1503303736806179\n",
      "Gradient Descent(619/999): loss=8.986842158294687, w0=-3.4722395845389356, w1=1.150484327154569\n",
      "Gradient Descent(620/999): loss=8.986605368572054, w0=-3.473766532071546, w1=1.1506377255967424\n",
      "Gradient Descent(621/999): loss=8.98637028311643, w0=-3.475287974665673, w1=1.1507905710081339\n",
      "Gradient Descent(622/999): loss=8.986136889661541, w0=-3.476803932167675, w1=1.1509428653825262\n",
      "Gradient Descent(623/999): loss=8.985905176029412, w0=-3.4783144243523605, w1=1.151094610706512\n",
      "Gradient Descent(624/999): loss=8.985675130129701, w0=-3.4798194709232453, w1=1.1512458089595248\n",
      "Gradient Descent(625/999): loss=8.985446739959103, w0=-3.481319091512811, w1=1.1513964621138588\n",
      "Gradient Descent(626/999): loss=8.985219993600692, w0=-3.4828133056827606, w1=1.1515465721347\n",
      "Gradient Descent(627/999): loss=8.984994879223327, w0=-3.484302132924272, w1=1.1516961409801487\n",
      "Gradient Descent(628/999): loss=8.984771385080991, w0=-3.4857855926582553, w1=1.1518451706012458\n",
      "Gradient Descent(629/999): loss=8.984549499512251, w0=-3.487263704235603, w1=1.1519936629419985\n",
      "Gradient Descent(630/999): loss=8.984329210939556, w0=-3.4887364869374458, w1=1.152141619939405\n",
      "Gradient Descent(631/999): loss=8.984110507868738, w0=-3.4902039599754002, w1=1.1522890435234814\n",
      "Gradient Descent(632/999): loss=8.983893378888308, w0=-3.4916661424918223, w1=1.1524359356172844\n",
      "Gradient Descent(633/999): loss=8.983677812668937, w0=-3.493123053560056, w1=1.1525822981369382\n",
      "Gradient Descent(634/999): loss=8.983463797962816, w0=-3.494574712184683, w1=1.1527281329916597\n",
      "Gradient Descent(635/999): loss=8.98325132360313, w0=-3.4960211373017684, w1=1.1528734420837816\n",
      "Gradient Descent(636/999): loss=8.9830403785034, w0=-3.49746234777911, w1=1.153018227308779\n",
      "Gradient Descent(637/999): loss=8.982830951656958, w0=-3.4988983624164836, w1=1.1531624905552933\n",
      "Gradient Descent(638/999): loss=8.982623032136335, w0=-3.5003291999458876, w1=1.153306233705157\n",
      "Gradient Descent(639/999): loss=8.982416609092732, w0=-3.5017548790317887, w1=1.153449458633418\n",
      "Gradient Descent(640/999): loss=8.98221167175543, w0=-3.5031754182713644, w1=1.1535921672083647\n",
      "Gradient Descent(641/999): loss=8.982008209431232, w0=-3.5045908361947453, w1=1.1537343612915496\n",
      "Gradient Descent(642/999): loss=8.98180621150388, w0=-3.5060011512652585, w1=1.153876042737814\n",
      "Gradient Descent(643/999): loss=8.981605667433564, w0=-3.5074063818796657, w1=1.1540172133953128\n",
      "Gradient Descent(644/999): loss=8.981406566756311, w0=-3.508806546368406, w1=1.1541578751055368\n",
      "Gradient Descent(645/999): loss=8.981208899083462, w0=-3.5102016629958332, w1=1.1542980297033387\n",
      "Gradient Descent(646/999): loss=8.981012654101127, w0=-3.5115917499604548, w1=1.154437679016956\n",
      "Gradient Descent(647/999): loss=8.980817821569653, w0=-3.5129768253951688, w1=1.1545768248680348\n",
      "Gradient Descent(648/999): loss=8.980624391323094, w0=-3.5143569073675014, w1=1.1547154690716541\n",
      "Gradient Descent(649/999): loss=8.980432353268663, w0=-3.515732013879841, w1=1.1548536134363494\n",
      "Gradient Descent(650/999): loss=8.980241697386205, w0=-3.517102162869675, w1=1.1549912597641356\n",
      "Gradient Descent(651/999): loss=8.980052413727705, w0=-3.5184673722098214, w1=1.1551284098505312\n",
      "Gradient Descent(652/999): loss=8.979864492416741, w0=-3.5198276597086644, w1=1.1552650654845817\n",
      "Gradient Descent(653/999): loss=8.979677923647966, w0=-3.521183043110385, w1=1.1554012284488822\n",
      "Gradient Descent(654/999): loss=8.97949269768661, w0=-3.5225335400951936, w1=1.1555369005196021\n",
      "Gradient Descent(655/999): loss=8.979308804867978, w0=-3.5238791682795587, w1=1.155672083466506\n",
      "Gradient Descent(656/999): loss=8.979126235596913, w0=-3.5252199452164397, w1=1.1558067790529798\n",
      "Gradient Descent(657/999): loss=8.978944980347345, w0=-3.5265558883955133, w1=1.155940989036051\n",
      "Gradient Descent(658/999): loss=8.978765029661744, w0=-3.5278870152434028, w1=1.1560747151664126\n",
      "Gradient Descent(659/999): loss=8.978586374150677, w0=-3.529213343123905, w1=1.156207959188447\n",
      "Gradient Descent(660/999): loss=8.978409004492258, w0=-3.5305348893382167, w1=1.1563407228402465\n",
      "Gradient Descent(661/999): loss=8.978232911431718, w0=-3.5318516711251613, w1=1.156473007853638\n",
      "Gradient Descent(662/999): loss=8.97805808578088, w0=-3.5331637056614125, w1=1.156604815954205\n",
      "Gradient Descent(663/999): loss=8.977884518417712, w0=-3.534471010061719, w1=1.1567361488613093\n",
      "Gradient Descent(664/999): loss=8.977712200285834, w0=-3.5357736013791268, w1=1.1568670082881145\n",
      "Gradient Descent(665/999): loss=8.977541122394046, w0=-3.5370714966052037, w1=1.156997395941608\n",
      "Gradient Descent(666/999): loss=8.977371275815862, w0=-3.5383647126702584, w1=1.1571273135226225\n",
      "Gradient Descent(667/999): loss=8.97720265168904, w0=-3.539653266443563, w1=1.15725676272586\n",
      "Gradient Descent(668/999): loss=8.977035241215125, w0=-3.5409371747335734, w1=1.1573857452399114\n",
      "Gradient Descent(669/999): loss=8.976869035658996, w0=-3.5422164542881465, w1=1.1575142627472816\n",
      "Gradient Descent(670/999): loss=8.97670402634838, w0=-3.543491121794761, w1=1.157642316924408\n",
      "Gradient Descent(671/999): loss=8.976540204673453, w0=-3.5447611938807335, w1=1.1577699094416856\n",
      "Gradient Descent(672/999): loss=8.976377562086338, w0=-3.5460266871134363, w1=1.1578970419634858\n",
      "Gradient Descent(673/999): loss=8.97621609010068, w0=-3.547287618000513, w1=1.1580237161481812\n",
      "Gradient Descent(674/999): loss=8.976055780291214, w0=-3.548544002990093, w1=1.1581499336481642\n",
      "Gradient Descent(675/999): loss=8.975896624293306, w0=-3.5497958584710094, w1=1.1582756961098717\n",
      "Gradient Descent(676/999): loss=8.975738613802527, w0=-3.551043200773008, w1=1.1584010051738027\n",
      "Gradient Descent(677/999): loss=8.975581740574222, w0=-3.552286046166964, w1=1.1585258624745443\n",
      "Gradient Descent(678/999): loss=8.975425996423077, w0=-3.5535244108650925, w1=1.158650269640788\n",
      "Gradient Descent(679/999): loss=8.975271373222684, w0=-3.554758311021161, w1=1.1587742282953566\n",
      "Gradient Descent(680/999): loss=8.975117862905138, w0=-3.555987762730699, w1=1.1588977400552187\n",
      "Gradient Descent(681/999): loss=8.974965457460575, w0=-3.5572127820312085, w1=1.1590208065315173\n",
      "Gradient Descent(682/999): loss=8.974814148936805, w0=-3.558433384902374, w1=1.1591434293295835\n",
      "Gradient Descent(683/999): loss=8.974663929438872, w0=-3.5596495872662692, w1=1.159265610048963\n",
      "Gradient Descent(684/999): loss=8.974514791128632, w0=-3.5608614049875666, w1=1.1593873502834344\n",
      "Gradient Descent(685/999): loss=8.974366726224355, w0=-3.562068853873743, w1=1.1595086516210296\n",
      "Gradient Descent(686/999): loss=8.974219727000335, w0=-3.563271949675286, w1=1.1596295156440564\n",
      "Gradient Descent(687/999): loss=8.974073785786455, w0=-3.5644707080858997, w1=1.1597499439291175\n",
      "Gradient Descent(688/999): loss=8.973928894967807, w0=-3.5656651447427103, w1=1.1598699380471316\n",
      "Gradient Descent(689/999): loss=8.973785046984284, w0=-3.566855275226468, w1=1.1599894995633544\n",
      "Gradient Descent(690/999): loss=8.97364223433021, w0=-3.568041115061752, w1=1.1601086300373982\n",
      "Gradient Descent(691/999): loss=8.973500449553926, w0=-3.5692226797171727, w1=1.1602273310232525\n",
      "Gradient Descent(692/999): loss=8.973359685257375, w0=-3.5703999846055723, w1=1.1603456040693048\n",
      "Gradient Descent(693/999): loss=8.97321993409579, w0=-3.5715730450842273, w1=1.1604634507183595\n",
      "Gradient Descent(694/999): loss=8.973081188777238, w0=-3.5727418764550483, w1=1.1605808725076603\n",
      "Gradient Descent(695/999): loss=8.972943442062286, w0=-3.5739064939647798, w1=1.1606978709689078\n",
      "Gradient Descent(696/999): loss=8.972806686763592, w0=-3.575066912805198, w1=1.16081444762828\n",
      "Gradient Descent(697/999): loss=8.97267091574556, w0=-3.576223148113311, w1=1.1609306040064546\n",
      "Gradient Descent(698/999): loss=8.972536121923927, w0=-3.5773752149715543, w1=1.1610463416186252\n",
      "Gradient Descent(699/999): loss=8.97240229826545, w0=-3.5785231284079884, w1=1.1611616619745238\n",
      "Gradient Descent(700/999): loss=8.972269437787475, w0=-3.579666903396495, w1=1.1612765665784388\n",
      "Gradient Descent(701/999): loss=8.972137533557634, w0=-3.5808065548569723, w1=1.1613910569292365\n",
      "Gradient Descent(702/999): loss=8.972006578693435, w0=-3.5819420976555287, w1=1.161505134520378\n",
      "Gradient Descent(703/999): loss=8.971876566361924, w0=-3.583073546604678, w1=1.1616188008399413\n",
      "Gradient Descent(704/999): loss=8.971747489779329, w0=-3.584200916463532, w1=1.1617320573706396\n",
      "Gradient Descent(705/999): loss=8.971619342210708, w0=-3.5853242219379924, w1=1.1618449055898394\n",
      "Gradient Descent(706/999): loss=8.971492116969596, w0=-3.586443477680944, w1=1.1619573469695832\n",
      "Gradient Descent(707/999): loss=8.971365807417628, w0=-3.5875586982924457, w1=1.1620693829766038\n",
      "Gradient Descent(708/999): loss=8.971240406964242, w0=-3.588669898319919, w1=1.1621810150723486\n",
      "Gradient Descent(709/999): loss=8.971115909066317, w0=-3.5897770922583394, w1=1.1622922447129946\n",
      "Gradient Descent(710/999): loss=8.970992307227803, w0=-3.5908802945504266, w1=1.1624030733494697\n",
      "Gradient Descent(711/999): loss=8.970869594999419, w0=-3.5919795195868303, w1=1.162513502427471\n",
      "Gradient Descent(712/999): loss=8.97074776597829, w0=-3.5930747817063193, w1=1.162623533387483\n",
      "Gradient Descent(713/999): loss=8.970626813807634, w0=-3.594166095195969, w1=1.1627331676647976\n",
      "Gradient Descent(714/999): loss=8.970506732176426, w0=-3.595253474291346, w1=1.1628424066895322\n",
      "Gradient Descent(715/999): loss=8.97038751481906, w0=-3.596336933176696, w1=1.1629512518866472\n",
      "Gradient Descent(716/999): loss=8.970269155515014, w0=-3.597416485985127, w1=1.1630597046759676\n",
      "Gradient Descent(717/999): loss=8.97015164808855, w0=-3.598492146798796, w1=1.1631677664721982\n",
      "Gradient Descent(718/999): loss=8.970034986408383, w0=-3.599563929649089, w1=1.1632754386849442\n",
      "Gradient Descent(719/999): loss=8.969919164387342, w0=-3.600631848516808, w1=1.1633827227187286\n",
      "Gradient Descent(720/999): loss=8.969804175982079, w0=-3.6016959173323495, w1=1.1634896199730111\n",
      "Gradient Descent(721/999): loss=8.969690015192741, w0=-3.60275614997589, w1=1.1635961318422057\n",
      "Gradient Descent(722/999): loss=8.969576676062658, w0=-3.6038125602775652, w1=1.1637022597156998\n",
      "Gradient Descent(723/999): loss=8.969464152678032, w0=-3.6048651620176493, w1=1.1638080049778712\n",
      "Gradient Descent(724/999): loss=8.969352439167615, w0=-3.605913968926737, w1=1.1639133690081074\n",
      "Gradient Descent(725/999): loss=8.969241529702446, w0=-3.6069589946859217, w1=1.1640183531808221\n",
      "Gradient Descent(726/999): loss=8.969131418495495, w0=-3.608000252926973, w1=1.164122958865475\n",
      "Gradient Descent(727/999): loss=8.969022099801387, w0=-3.6090377572325156, w1=1.1642271874265875\n",
      "Gradient Descent(728/999): loss=8.968913567916116, w0=-3.6100715211362067, w1=1.1643310402237628\n",
      "Gradient Descent(729/999): loss=8.968805817176694, w0=-3.611101558122912, w1=1.1644345186117013\n",
      "Gradient Descent(730/999): loss=8.968698841960926, w0=-3.612127881628881, w1=1.164537623940221\n",
      "Gradient Descent(731/999): loss=8.968592636687058, w0=-3.613150505041924, w1=1.1646403575542714\n",
      "Gradient Descent(732/999): loss=8.96848719581353, w0=-3.6141694417015846, w1=1.1647427207939556\n",
      "Gradient Descent(733/999): loss=8.96838251383867, w0=-3.6151847048993155, w1=1.1648447149945431\n",
      "Gradient Descent(734/999): loss=8.968278585300359, w0=-3.616196307878651, w1=1.1649463414864913\n",
      "Gradient Descent(735/999): loss=8.968175404775852, w0=-3.6172042638353794, w1=1.16504760159546\n",
      "Gradient Descent(736/999): loss=8.968072966881396, w0=-3.6182085859177167, w1=1.16514849664233\n",
      "Gradient Descent(737/999): loss=8.967971266271984, w0=-3.619209287226476, w1=1.1652490279432206\n",
      "Gradient Descent(738/999): loss=8.967870297641115, w0=-3.6202063808152407, w1=1.1653491968095049\n",
      "Gradient Descent(739/999): loss=8.967770055720441, w0=-3.621199879690532, w1=1.16544900454783\n",
      "Gradient Descent(740/999): loss=8.967670535279542, w0=-3.622189796811981, w1=1.1655484524601307\n",
      "Gradient Descent(741/999): loss=8.967571731125677, w0=-3.623176145092497, w1=1.1656475418436496\n",
      "Gradient Descent(742/999): loss=8.967473638103415, w0=-3.6241589373984358, w1=1.1657462739909505\n",
      "Gradient Descent(743/999): loss=8.967376251094494, w0=-3.6251381865497665, w1=1.1658446501899398\n",
      "Gradient Descent(744/999): loss=8.967279565017458, w0=-3.6261139053202407, w1=1.165942671723878\n",
      "Gradient Descent(745/999): loss=8.967183574827422, w0=-3.627086106437558, w1=1.1660403398714019\n",
      "Gradient Descent(746/999): loss=8.967088275515827, w0=-3.6280548025835326, w1=1.1661376559065362\n",
      "Gradient Descent(747/999): loss=8.966993662110156, w0=-3.6290200063942573, w1=1.166234621098714\n",
      "Gradient Descent(748/999): loss=8.966899729673672, w0=-3.62998173046027, w1=1.1663312367127914\n",
      "Gradient Descent(749/999): loss=8.966806473305176, w0=-3.6309399873267174, w1=1.1664275040090644\n",
      "Gradient Descent(750/999): loss=8.966713888138758, w0=-3.6318947894935185, w1=1.1665234242432854\n",
      "Gradient Descent(751/999): loss=8.966621969343503, w0=-3.6328461494155273, w1=1.1666189986666795\n",
      "Gradient Descent(752/999): loss=8.966530712123287, w0=-3.6337940795026964, w1=1.1667142285259613\n",
      "Gradient Descent(753/999): loss=8.9664401117165, w0=-3.6347385921202373, w1=1.1668091150633502\n",
      "Gradient Descent(754/999): loss=8.9663501633958, w0=-3.635679699588783, w1=1.1669036595165874\n",
      "Gradient Descent(755/999): loss=8.966260862467877, w0=-3.6366174141845486, w1=1.1669978631189522\n",
      "Gradient Descent(756/999): loss=8.966172204273182, w0=-3.6375517481394906, w1=1.1670917270992769\n",
      "Gradient Descent(757/999): loss=8.96608418418574, w0=-3.6384827136414666, w1=1.1671852526819642\n",
      "Gradient Descent(758/999): loss=8.965996797612823, w0=-3.639410322834395, w1=1.1672784410870023\n",
      "Gradient Descent(759/999): loss=8.96591003999479, w0=-3.6403345878184137, w1=1.1673712935299816\n",
      "Gradient Descent(760/999): loss=8.965823906804815, w0=-3.6412555206500365, w1=1.1674638112221094\n",
      "Gradient Descent(761/999): loss=8.965738393548651, w0=-3.6421731333423115, w1=1.1675559953702266\n",
      "Gradient Descent(762/999): loss=8.965653495764387, w0=-3.643087437864977, w1=1.1676478471768237\n",
      "Gradient Descent(763/999): loss=8.96556920902223, w0=-3.643998446144619, w1=1.167739367840055\n",
      "Gradient Descent(764/999): loss=8.965485528924287, w0=-3.644906170064824, w1=1.1678305585537563\n",
      "Gradient Descent(765/999): loss=8.965402451104294, w0=-3.645810621466339, w1=1.1679214205074586\n",
      "Gradient Descent(766/999): loss=8.965319971227435, w0=-3.6467118121472195, w1=1.168011954886405\n",
      "Gradient Descent(767/999): loss=8.96523808499007, w0=-3.647609753862989, w1=1.1681021628715647\n",
      "Gradient Descent(768/999): loss=8.965156788119556, w0=-3.6485044583267894, w1=1.1681920456396504\n",
      "Gradient Descent(769/999): loss=8.965076076373997, w0=-3.649395937209534, w1=1.1682816043631317\n",
      "Gradient Descent(770/999): loss=8.964995945542007, w0=-3.650284202140061, w1=1.1683708402102517\n",
      "Gradient Descent(771/999): loss=8.964916391442545, w0=-3.6511692647052842, w1=1.1684597543450412\n",
      "Gradient Descent(772/999): loss=8.964837409924623, w0=-3.652051136450344, w1=1.1685483479273353\n",
      "Gradient Descent(773/999): loss=8.964758996867172, w0=-3.652929828878759, w1=1.1686366221127868\n",
      "Gradient Descent(774/999): loss=8.96468114817875, w0=-3.653805353452574, w1=1.1687245780528825\n",
      "Gradient Descent(775/999): loss=8.964603859797379, w0=-3.654677721592513, w1=1.1688122168949584\n",
      "Gradient Descent(776/999): loss=8.964527127690317, w0=-3.6555469446781244, w1=1.168899539782213\n",
      "Gradient Descent(777/999): loss=8.964450947853832, w0=-3.656413034047932, w1=1.1689865478537245\n",
      "Gradient Descent(778/999): loss=8.964375316313022, w0=-3.657276000999582, w1=1.1690732422444634\n",
      "Gradient Descent(779/999): loss=8.9643002291216, w0=-3.65813585678999, w1=1.16915962408531\n",
      "Gradient Descent(780/999): loss=8.964225682361675, w0=-3.658992612635489, w1=1.1692456945030651\n",
      "Gradient Descent(781/999): loss=8.964151672143545, w0=-3.6598462797119735, w1=1.16933145462047\n",
      "Gradient Descent(782/999): loss=8.96407819460551, w0=-3.6606968691550485, w1=1.169416905556216\n",
      "Gradient Descent(783/999): loss=8.964005245913675, w0=-3.661544392060172, w1=1.169502048424963\n",
      "Gradient Descent(784/999): loss=8.963932822261716, w0=-3.662388859482801, w1=1.169586884337351\n",
      "Gradient Descent(785/999): loss=8.963860919870735, w0=-3.663230282438535, w1=1.1696714144000162\n",
      "Gradient Descent(786/999): loss=8.963789534989003, w0=-3.664068671903262, w1=1.1697556397156068\n",
      "Gradient Descent(787/999): loss=8.96371866389181, w0=-3.664904038813297, w1=1.1698395613827928\n",
      "Gradient Descent(788/999): loss=8.963648302881264, w0=-3.6657363940655294, w1=1.169923180496286\n",
      "Gradient Descent(789/999): loss=8.963578448286045, w0=-3.666565748517563, w1=1.1700064981468499\n",
      "Gradient Descent(790/999): loss=8.963509096461317, w0=-3.6673921129878573, w1=1.1700895154213162\n",
      "Gradient Descent(791/999): loss=8.96344024378842, w0=-3.6682154982558695, w1=1.1701722334025988\n",
      "Gradient Descent(792/999): loss=8.963371886674786, w0=-3.669035915062195, w1=1.1702546531697062\n",
      "Gradient Descent(793/999): loss=8.963304021553672, w0=-3.6698533741087065, w1=1.1703367757977583\n",
      "Gradient Descent(794/999): loss=8.96323664488402, w0=-3.6706678860586957, w1=1.1704186023579979\n",
      "Gradient Descent(795/999): loss=8.963169753150266, w0=-3.6714794615370097, w1=1.1705001339178067\n",
      "Gradient Descent(796/999): loss=8.963103342862127, w0=-3.672288111130192, w1=1.170581371540717\n",
      "Gradient Descent(797/999): loss=8.963037410554467, w0=-3.6730938453866195, w1=1.1706623162864285\n",
      "Gradient Descent(798/999): loss=8.962971952787056, w0=-3.6738966748166395, w1=1.1707429692108189\n",
      "Gradient Descent(799/999): loss=8.962906966144457, w0=-3.674696609892708, w1=1.1708233313659608\n",
      "Gradient Descent(800/999): loss=8.962842447235811, w0=-3.6754936610495252, w1=1.1709034038001322\n",
      "Gradient Descent(801/999): loss=8.962778392694641, w0=-3.676287838684173, w1=1.1709831875578331\n",
      "Gradient Descent(802/999): loss=8.962714799178723, w0=-3.67707915315625, w1=1.1710626836797977\n",
      "Gradient Descent(803/999): loss=8.962651663369874, w0=-3.6778676147880054, w1=1.1711418932030078\n",
      "Gradient Descent(804/999): loss=8.96258898197381, w0=-3.6786532338644755, w1=1.1712208171607068\n",
      "Gradient Descent(805/999): loss=8.962526751719935, w0=-3.679436020633617, w1=1.171299456582413\n",
      "Gradient Descent(806/999): loss=8.962464969361216, w0=-3.6802159853064405, w1=1.1713778124939334\n",
      "Gradient Descent(807/999): loss=8.962403631673967, w0=-3.680993138057144, w1=1.1714558859173763\n",
      "Gradient Descent(808/999): loss=8.962342735457725, w0=-3.6817674890232452, w1=1.1715336778711656\n",
      "Gradient Descent(809/999): loss=8.96228227753505, w0=-3.682539048305715, w1=1.1716111893700532\n",
      "Gradient Descent(810/999): loss=8.962222254751373, w0=-3.6833078259691083, w1=1.1716884214251329\n",
      "Gradient Descent(811/999): loss=8.962162663974834, w0=-3.6840738320416944, w1=1.1717653750438528\n",
      "Gradient Descent(812/999): loss=8.962103502096104, w0=-3.6848370765155893, w1=1.17184205123003\n",
      "Gradient Descent(813/999): loss=8.962044766028253, w0=-3.6855975693468856, w1=1.1719184509838612\n",
      "Gradient Descent(814/999): loss=8.961986452706538, w0=-3.686355320455782, w1=1.171994575301939\n",
      "Gradient Descent(815/999): loss=8.961928559088303, w0=-3.687110339726714, w1=1.1720704251772607\n",
      "Gradient Descent(816/999): loss=8.961871082152784, w0=-3.68786263700848, w1=1.1721460015992464\n",
      "Gradient Descent(817/999): loss=8.961814018900945, w0=-3.6886122221143736, w1=1.1722213055537467\n",
      "Gradient Descent(818/999): loss=8.96175736635535, w0=-3.6893591048223073, w1=1.1722963380230602\n",
      "Gradient Descent(819/999): loss=8.961701121559983, w0=-3.6901032948749446, w1=1.1723710999859416\n",
      "Gradient Descent(820/999): loss=8.961645281580115, w0=-3.6908448019798237, w1=1.17244559241762\n",
      "Gradient Descent(821/999): loss=8.961589843502129, w0=-3.691583635809485, w1=1.1725198162898058\n",
      "Gradient Descent(822/999): loss=8.961534804433386, w0=-3.6923198060015987, w1=1.172593772570708\n",
      "Gradient Descent(823/999): loss=8.961480161502058, w0=-3.693053322159088, w1=1.1726674622250446\n",
      "Gradient Descent(824/999): loss=8.961425911856988, w0=-3.693784193850257, w1=1.172740886214055\n",
      "Gradient Descent(825/999): loss=8.961372052667551, w0=-3.694512430608913, w1=1.172814045495514\n",
      "Gradient Descent(826/999): loss=8.96131858112348, w0=-3.695238041934493, w1=1.1728869410237426\n",
      "Gradient Descent(827/999): loss=8.96126549443475, w0=-3.695961037292186, w1=1.1729595737496226\n",
      "Gradient Descent(828/999): loss=8.961212789831407, w0=-3.6966814261130576, w1=1.1730319446206057\n",
      "Gradient Descent(829/999): loss=8.961160464563418, w0=-3.697399217794173, w1=1.1731040545807299\n",
      "Gradient Descent(830/999): loss=8.961108515900582, w0=-3.6981144216987185, w1=1.1731759045706285\n",
      "Gradient Descent(831/999): loss=8.961056941132325, w0=-3.6988270471561244, w1=1.1732474955275438\n",
      "Gradient Descent(832/999): loss=8.961005737567563, w0=-3.699537103462187, w1=1.1733188283853397\n",
      "Gradient Descent(833/999): loss=8.960954902534622, w0=-3.7002445998791895, w1=1.1733899040745128\n",
      "Gradient Descent(834/999): loss=8.960904433381025, w0=-3.700949545636022, w1=1.1734607235222052\n",
      "Gradient Descent(835/999): loss=8.960854327473413, w0=-3.7016519499283036, w1=1.173531287652217\n",
      "Gradient Descent(836/999): loss=8.960804582197348, w0=-3.702351821918501, w1=1.1736015973850165\n",
      "Gradient Descent(837/999): loss=8.960755194957224, w0=-3.7030491707360484, w1=1.1736716536377554\n",
      "Gradient Descent(838/999): loss=8.960706163176127, w0=-3.703744005477467, w1=1.173741457324277\n",
      "Gradient Descent(839/999): loss=8.96065748429568, w0=-3.7044363352064824, w1=1.1738110093551317\n",
      "Gradient Descent(840/999): loss=8.960609155775929, w0=-3.705126168954145, w1=1.1738803106375857\n",
      "Gradient Descent(841/999): loss=8.960561175095172, w0=-3.7058135157189454, w1=1.1739493620756356\n",
      "Gradient Descent(842/999): loss=8.960513539749886, w0=-3.706498384466934, w1=1.174018164570018\n",
      "Gradient Descent(843/999): loss=8.960466247254576, w0=-3.7071807841318365, w1=1.1740867190182227\n",
      "Gradient Descent(844/999): loss=8.960419295141586, w0=-3.7078607236151697, w1=1.1741550263145037\n",
      "Gradient Descent(845/999): loss=8.960372680961079, w0=-3.7085382117863603, w1=1.1742230873498911\n",
      "Gradient Descent(846/999): loss=8.960326402280824, w0=-3.7092132574828582, w1=1.1742909030122024\n",
      "Gradient Descent(847/999): loss=8.960280456686078, w0=-3.7098858695102526, w1=1.1743584741860549\n",
      "Gradient Descent(848/999): loss=8.960234841779512, w0=-3.710556056642387, w1=1.1744258017528757\n",
      "Gradient Descent(849/999): loss=8.960189555181033, w0=-3.711223827621474, w1=1.1744928865909159\n",
      "Gradient Descent(850/999): loss=8.960144594527678, w0=-3.7118891911582077, w1=1.1745597295752581\n",
      "Gradient Descent(851/999): loss=8.960099957473489, w0=-3.7125521559318795, w1=1.174626331577832\n",
      "Gradient Descent(852/999): loss=8.96005564168941, w0=-3.71321273059049, w1=1.1746926934674229\n",
      "Gradient Descent(853/999): loss=8.960011644863126, w0=-3.7138709237508616, w1=1.174758816109684\n",
      "Gradient Descent(854/999): loss=8.959967964698974, w0=-3.7145267439987526, w1=1.174824700367148\n",
      "Gradient Descent(855/999): loss=8.959924598917821, w0=-3.715180199888967, w1=1.1748903470992371\n",
      "Gradient Descent(856/999): loss=8.959881545256932, w0=-3.715831299945467, w1=1.1749557571622768\n",
      "Gradient Descent(857/999): loss=8.959838801469854, w0=-3.7164800526614847, w1=1.1750209314095037\n",
      "Gradient Descent(858/999): loss=8.959796365326303, w0=-3.7171264664996326, w1=1.1750858706910792\n",
      "Gradient Descent(859/999): loss=8.959754234612053, w0=-3.7177705498920135, w1=1.1751505758540997\n",
      "Gradient Descent(860/999): loss=8.959712407128812, w0=-3.718412311240331, w1=1.1752150477426069\n",
      "Gradient Descent(861/999): loss=8.959670880694107, w0=-3.7190517589159993, w1=1.1752792871976006\n",
      "Gradient Descent(862/999): loss=8.959629653141173, w0=-3.719688901260251, w1=1.1753432950570484\n",
      "Gradient Descent(863/999): loss=8.959588722318852, w0=-3.7203237465842482, w1=1.1754070721558953\n",
      "Gradient Descent(864/999): loss=8.95954808609145, w0=-3.720956303169189, w1=1.1754706193260795\n",
      "Gradient Descent(865/999): loss=8.95950774233866, w0=-3.7215865792664165, w1=1.175533937396536\n",
      "Gradient Descent(866/999): loss=8.95946768895543, w0=-3.7222145830975255, w1=1.1755970271932148\n",
      "Gradient Descent(867/999): loss=8.95942792385185, w0=-3.722840322854471, w1=1.1756598895390855\n",
      "Gradient Descent(868/999): loss=8.959388444953065, w0=-3.7234638066996744, w1=1.1757225252541523\n",
      "Gradient Descent(869/999): loss=8.959349250199148, w0=-3.72408504276613, w1=1.1757849351554626\n",
      "Gradient Descent(870/999): loss=8.959310337544997, w0=-3.72470403915751, w1=1.1758471200571183\n",
      "Gradient Descent(871/999): loss=8.95927170496024, w0=-3.7253208039482737, w1=1.175909080770286\n",
      "Gradient Descent(872/999): loss=8.959233350429095, w0=-3.725935345183768, w1=1.175970818103209\n",
      "Gradient Descent(873/999): loss=8.95919527195032, w0=-3.7265476708803362, w1=1.1760323328612146\n",
      "Gradient Descent(874/999): loss=8.959157467537041, w0=-3.7271577890254206, w1=1.1760936258467294\n",
      "Gradient Descent(875/999): loss=8.959119935216723, w0=-3.7277657075776673, w1=1.1761546978592847\n",
      "Gradient Descent(876/999): loss=8.959082673030995, w0=-3.72837143446703, w1=1.1762155496955307\n",
      "Gradient Descent(877/999): loss=8.959045679035592, w0=-3.7289749775948735, w1=1.1762761821492453\n",
      "Gradient Descent(878/999): loss=8.959008951300246, w0=-3.7295763448340766, w1=1.1763365960113445\n",
      "Gradient Descent(879/999): loss=8.958972487908591, w0=-3.7301755440291346, w1=1.1763967920698932\n",
      "Gradient Descent(880/999): loss=8.95893628695803, w0=-3.7307725829962624, w1=1.1764567711101144\n",
      "Gradient Descent(881/999): loss=8.95890034655968, w0=-3.7313674695234953, w1=1.1765165339144013\n",
      "Gradient Descent(882/999): loss=8.958864664838245, w0=-3.7319602113707924, w1=1.1765760812623252\n",
      "Gradient Descent(883/999): loss=8.958829239931937, w0=-3.7325508162701353, w1=1.1766354139306487\n",
      "Gradient Descent(884/999): loss=8.958794069992342, w0=-3.7331392919256308, w1=1.1766945326933316\n",
      "Gradient Descent(885/999): loss=8.958759153184378, w0=-3.7337256460136112, w1=1.1767534383215457\n",
      "Gradient Descent(886/999): loss=8.95872448768615, w0=-3.734309886182734, w1=1.176812131583681\n",
      "Gradient Descent(887/999): loss=8.958690071688892, w0=-3.734892020054082, w1=1.1768706132453586\n",
      "Gradient Descent(888/999): loss=8.958655903396844, w0=-3.735472055221262, w1=1.1769288840694376\n",
      "Gradient Descent(889/999): loss=8.958621981027171, w0=-3.736049999250505, w1=1.1769869448160295\n",
      "Gradient Descent(890/999): loss=8.9585883028099, w0=-3.736625859680764, w1=1.1770447962425021\n",
      "Gradient Descent(891/999): loss=8.958554866987747, w0=-3.7371996440238124, w1=1.1771024391034963\n",
      "Gradient Descent(892/999): loss=8.958521671816108, w0=-3.7377713597643427, w1=1.1771598741509293\n",
      "Gradient Descent(893/999): loss=8.958488715562925, w0=-3.738341014360063, w1=1.1772171021340094\n",
      "Gradient Descent(894/999): loss=8.958455996508617, w0=-3.7389086152417956, w1=1.1772741237992432\n",
      "Gradient Descent(895/999): loss=8.95842351294597, w0=-3.7394741698135734, w1=1.177330939890446\n",
      "Gradient Descent(896/999): loss=8.958391263180054, w0=-3.7400376854527355, w1=1.1773875511487517\n",
      "Gradient Descent(897/999): loss=8.958359245528142, w0=-3.7405991695100247, w1=1.1774439583126222\n",
      "Gradient Descent(898/999): loss=8.958327458319637, w0=-3.741158629309683, w1=1.1775001621178565\n",
      "Gradient Descent(899/999): loss=8.958295899895933, w0=-3.741716072149547, w1=1.1775561632976024\n",
      "Gradient Descent(900/999): loss=8.958264568610387, w0=-3.7422715053011437, w1=1.1776119625823627\n",
      "Gradient Descent(901/999): loss=8.95823346282819, w0=-3.742824936009784, w1=1.1776675607000082\n",
      "Gradient Descent(902/999): loss=8.958202580926304, w0=-3.743376371494659, w1=1.1777229583757844\n",
      "Gradient Descent(903/999): loss=8.958171921293392, w0=-3.7439258189489326, w1=1.1777781563323229\n",
      "Gradient Descent(904/999): loss=8.958141482329676, w0=-3.744473285539836, w1=1.1778331552896502\n",
      "Gradient Descent(905/999): loss=8.958111262446936, w0=-3.745018778408761, w1=1.1778879559651956\n",
      "Gradient Descent(906/999): loss=8.958081260068361, w0=-3.745562304671354, w1=1.177942559073804\n",
      "Gradient Descent(907/999): loss=8.958051473628473, w0=-3.7461038714176076, w1=1.1779969653277411\n",
      "Gradient Descent(908/999): loss=8.95802190157309, w0=-3.7466434857119535, w1=1.1780511754367067\n",
      "Gradient Descent(909/999): loss=8.957992542359204, w0=-3.7471811545933553, w1=1.17810519010784\n",
      "Gradient Descent(910/999): loss=8.957963394454897, w0=-3.7477168850753992, w1=1.1781590100457329\n",
      "Gradient Descent(911/999): loss=8.957934456339318, w0=-3.7482506841463867, w1=1.178212635952435\n",
      "Gradient Descent(912/999): loss=8.957905726502512, w0=-3.748782558769425, w1=1.1782660685274668\n",
      "Gradient Descent(913/999): loss=8.957877203445443, w0=-3.749312515882517, w1=1.1783193084678254\n",
      "Gradient Descent(914/999): loss=8.957848885679807, w0=-3.749840562398654, w1=1.178372356467996\n",
      "Gradient Descent(915/999): loss=8.957820771728064, w0=-3.7503667052059044, w1=1.17842521321996\n",
      "Gradient Descent(916/999): loss=8.957792860123279, w0=-3.750890951167503, w1=1.1784778794132034\n",
      "Gradient Descent(917/999): loss=8.957765149409084, w0=-3.7514133071219424, w1=1.1785303557347269\n",
      "Gradient Descent(918/999): loss=8.957737638139593, w0=-3.75193377988306, w1=1.178582642869055\n",
      "Gradient Descent(919/999): loss=8.957710324879322, w0=-3.752452376240129, w1=1.178634741498243\n",
      "Gradient Descent(920/999): loss=8.957683208203132, w0=-3.752969102957946, w1=1.1786866523018886\n",
      "Gradient Descent(921/999): loss=8.957656286696114, w0=-3.753483966776918, w1=1.1787383759571386\n",
      "Gradient Descent(922/999): loss=8.957629558953576, w0=-3.753996974413153, w1=1.1787899131386994\n",
      "Gradient Descent(923/999): loss=8.957603023580916, w0=-3.7545081325585454, w1=1.1788412645188433\n",
      "Gradient Descent(924/999): loss=8.957576679193572, w0=-3.755017447880864, w1=1.1788924307674213\n",
      "Gradient Descent(925/999): loss=8.957550524416952, w0=-3.755524927023839, w1=1.1789434125518672\n",
      "Gradient Descent(926/999): loss=8.957524557886343, w0=-3.7560305766072486, w1=1.1789942105372102\n",
      "Gradient Descent(927/999): loss=8.957498778246881, w0=-3.7565344032270063, w1=1.1790448253860806\n",
      "Gradient Descent(928/999): loss=8.957473184153432, w0=-3.757036413455245, w1=1.179095257758721\n",
      "Gradient Descent(929/999): loss=8.95744777427055, w0=-3.7575366138404043, w1=1.179145508312993\n",
      "Gradient Descent(930/999): loss=8.957422547272394, w0=-3.7580350109073155, w1=1.1791955777043868\n",
      "Gradient Descent(931/999): loss=8.957397501842687, w0=-3.7585316111572866, w1=1.179245466586029\n",
      "Gradient Descent(932/999): loss=8.957372636674606, w0=-3.7590264210681865, w1=1.1792951756086925\n",
      "Gradient Descent(933/999): loss=8.957347950470723, w0=-3.7595194470945312, w1=1.1793447054208022\n",
      "Gradient Descent(934/999): loss=8.957323441942977, w0=-3.760010695667566, w1=1.179394056668448\n",
      "Gradient Descent(935/999): loss=8.957299109812563, w0=-3.760500173195351, w1=1.1794432299953876\n",
      "Gradient Descent(936/999): loss=8.95727495280988, w0=-3.760987886062843, w1=1.1794922260430603\n",
      "Gradient Descent(937/999): loss=8.957250969674476, w0=-3.761473840631982, w1=1.1795410454505912\n",
      "Gradient Descent(938/999): loss=8.957227159154952, w0=-3.761958043241769, w1=1.1795896888548023\n",
      "Gradient Descent(939/999): loss=8.957203520008907, w0=-3.762440500208354, w1=1.1796381568902188\n",
      "Gradient Descent(940/999): loss=8.957180051002931, w0=-3.7629212178251152, w1=1.1796864501890794\n",
      "Gradient Descent(941/999): loss=8.957156750912443, w0=-3.763400202362742, w1=1.1797345693813421\n",
      "Gradient Descent(942/999): loss=8.957133618521704, w0=-3.763877460069317, w1=1.1797825150946954\n",
      "Gradient Descent(943/999): loss=8.9571106526237, w0=-3.7643529971703966, w1=1.1798302879545632\n",
      "Gradient Descent(944/999): loss=8.957087852020134, w0=-3.7648268198690937, w1=1.1798778885841166\n",
      "Gradient Descent(945/999): loss=8.957065215521299, w0=-3.7652989343461574, w1=1.1799253176042772\n",
      "Gradient Descent(946/999): loss=8.95704274194609, w0=-3.765769346760054, w1=1.1799725756337316\n",
      "Gradient Descent(947/999): loss=8.95702043012187, w0=-3.7662380632470476, w1=1.1800196632889326\n",
      "Gradient Descent(948/999): loss=8.956998278884464, w0=-3.7667050899212793, w1=1.180066581184113\n",
      "Gradient Descent(949/999): loss=8.956976287078058, w0=-3.7671704328748485, w1=1.1801133299312891\n",
      "Gradient Descent(950/999): loss=8.956954453555168, w0=-3.7676340981778904, w1=1.1801599101402733\n",
      "Gradient Descent(951/999): loss=8.956932777176569, w0=-3.768096091878657, w1=1.180206322418677\n",
      "Gradient Descent(952/999): loss=8.956911256811226, w0=-3.7685564200035944, w1=1.1802525673719226\n",
      "Gradient Descent(953/999): loss=8.956889891336267, w0=-3.7690150885574227, w1=1.1802986456032492\n",
      "Gradient Descent(954/999): loss=8.956868679636862, w0=-3.7694721035232144, w1=1.1803445577137215\n",
      "Gradient Descent(955/999): loss=8.956847620606254, w0=-3.769927470862471, w1=1.1803903043022366\n",
      "Gradient Descent(956/999): loss=8.956826713145613, w0=-3.7703811965152014, w1=1.180435885965533\n",
      "Gradient Descent(957/999): loss=8.95680595616403, w0=-3.7708332864000007, w1=1.1804813032981982\n",
      "Gradient Descent(958/999): loss=8.956785348578455, w0=-3.7712837464141256, w1=1.180526556892675\n",
      "Gradient Descent(959/999): loss=8.956764889313622, w0=-3.771732582433572, w1=1.1805716473392713\n",
      "Gradient Descent(960/999): loss=8.956744577302018, w0=-3.7721798003131526, w1=1.1806165752261661\n",
      "Gradient Descent(961/999): loss=8.956724411483801, w0=-3.772625405886571, w1=1.1806613411394191\n",
      "Gradient Descent(962/999): loss=8.956704390806749, w0=-3.7730694049665003, w1=1.1807059456629752\n",
      "Gradient Descent(963/999): loss=8.956684514226248, w0=-3.7735118033446575, w1=1.1807503893786762\n",
      "Gradient Descent(964/999): loss=8.956664780705163, w0=-3.773952606791879, w1=1.1807946728662644\n",
      "Gradient Descent(965/999): loss=8.95664518921384, w0=-3.7743918210581966, w1=1.1808387967033935\n",
      "Gradient Descent(966/999): loss=8.956625738730049, w0=-3.774829451872912, w1=1.1808827614656334\n",
      "Gradient Descent(967/999): loss=8.956606428238885, w0=-3.7752655049446715, w1=1.1809265677264806\n",
      "Gradient Descent(968/999): loss=8.95658725673278, w0=-3.7756999859615408, w1=1.1809702160573616\n",
      "Gradient Descent(969/999): loss=8.956568223211397, w0=-3.7761329005910795, w1=1.1810137070276452\n",
      "Gradient Descent(970/999): loss=8.956549326681627, w0=-3.7765642544804137, w1=1.1810570412046462\n",
      "Gradient Descent(971/999): loss=8.956530566157461, w0=-3.776994053256311, w1=1.1811002191536344\n",
      "Gradient Descent(972/999): loss=8.956511940660032, w0=-3.7774223025252534, w1=1.1811432414378418\n",
      "Gradient Descent(973/999): loss=8.956493449217493, w0=-3.7778490078735105, w1=1.1811861086184696\n",
      "Gradient Descent(974/999): loss=8.956475090865002, w0=-3.7782741748672124, w1=1.1812288212546962\n",
      "Gradient Descent(975/999): loss=8.956456864644661, w0=-3.7786978090524217, w1=1.1812713799036836\n",
      "Gradient Descent(976/999): loss=8.956438769605453, w0=-3.779119915955207, w1=1.1813137851205853\n",
      "Gradient Descent(977/999): loss=8.956420804803237, w0=-3.779540501081714, w1=1.1813560374585534\n",
      "Gradient Descent(978/999): loss=8.956402969300623, w0=-3.779959569918238, w1=1.181398137468746\n",
      "Gradient Descent(979/999): loss=8.956385262167009, w0=-3.7803771279312945, w1=1.1814400857003333\n",
      "Gradient Descent(980/999): loss=8.95636768247847, w0=-3.7807931805676924, w1=1.181481882700507\n",
      "Gradient Descent(981/999): loss=8.956350229317723, w0=-3.7812077332546026, w1=1.181523529014485\n",
      "Gradient Descent(982/999): loss=8.95633290177412, w0=-3.7816207913996305, w1=1.1815650251855196\n",
      "Gradient Descent(983/999): loss=8.956315698943529, w0=-3.782032360390886, w1=1.1816063717549055\n",
      "Gradient Descent(984/999): loss=8.956298619928353, w0=-3.782442445597054, w1=1.1816475692619848\n",
      "Gradient Descent(985/999): loss=8.956281663837439, w0=-3.782851052367464, w1=1.181688618244156\n",
      "Gradient Descent(986/999): loss=8.956264829786049, w0=-3.7832581860321604, w1=1.1817295192368795\n",
      "Gradient Descent(987/999): loss=8.956248116895827, w0=-3.783663851901971, w1=1.1817702727736863\n",
      "Gradient Descent(988/999): loss=8.956231524294722, w0=-3.784068055268578, w1=1.1818108793861823\n",
      "Gradient Descent(989/999): loss=8.956215051116978, w0=-3.784470801404586, w1=1.1818513396040586\n",
      "Gradient Descent(990/999): loss=8.95619869650304, w0=-3.7848720955635904, w1=1.1818916539550954\n",
      "Gradient Descent(991/999): loss=8.956182459599576, w0=-3.7852719429802466, w1=1.1819318229651712\n",
      "Gradient Descent(992/999): loss=8.956166339559363, w0=-3.785670348870338, w1=1.181971847158267\n",
      "Gradient Descent(993/999): loss=8.956150335541308, w0=-3.786067318430844, w1=1.1820117270564767\n",
      "Gradient Descent(994/999): loss=8.956134446710347, w0=-3.786462856840008, w1=1.1820514631800105\n",
      "Gradient Descent(995/999): loss=8.956118672237432, w0=-3.7868569692574052, w1=1.1820910560472038\n",
      "Gradient Descent(996/999): loss=8.95610301129949, w0=-3.787249660824009, w1=1.1821305061745235\n",
      "Gradient Descent(997/999): loss=8.956087463079351, w0=-3.7876409366622585, w1=1.1821698140765735\n",
      "Gradient Descent(998/999): loss=8.956072026765757, w0=-3.788030801876126, w1=1.1822089802661038\n",
      "Gradient Descent(999/999): loss=8.956056701553274, w0=-3.7884192615511827, w1=1.1822480052540147\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-3.78841926,  1.18224801])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g, cost = gradientDescent(X, y, theta, alpha, iters)\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can compute the cost (error) of the trained model using our fitted parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.956041486642254"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeCost(X, y, g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the linear model along with the data to visually see how well it fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Predicted Profit vs. Population Size')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(data.Population.min(), data.Population.max(), 100)\n",
    "f = g[0] + (g[1] * x)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.plot(x, f, 'r', label='Prediction')\n",
    "ax.scatter(data.Population, data.Profit, label='Traning Data')\n",
    "ax.legend(loc=2)\n",
    "ax.set_xlabel('Population')\n",
    "ax.set_ylabel('Profit')\n",
    "ax.set_title('Predicted Profit vs. Population Size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks pretty good!  Since the gradient decent function also outputs a vector with the cost at each training iteration, we can plot that as well.  Notice that the cost always decreases - this is an example of a convex optimization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Error vs. Training Epoch')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.plot(np.arange(iters), cost, 'r')\n",
    "ax.set_xlabel('Iterations')\n",
    "ax.set_ylabel('Cost')\n",
    "ax.set_title('Error vs. Training Epoch')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
